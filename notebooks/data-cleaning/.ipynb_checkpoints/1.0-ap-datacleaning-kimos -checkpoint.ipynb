{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text pre-processing\n",
    "\n",
    "The following basic preprocessing will be examined in this notebook. A similar example for this study can be found at [Analytics Vidhya's Website](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/).\n",
    "\n",
    "- Lower casing\n",
    "- Punctuation removal\n",
    "- Stopwords removal\n",
    "- Frequent words removal\n",
    "- Rare words removal\n",
    "- Spelling correction\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from nltk.stem import PorterStemmer\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "except:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imports\n",
    "json_data = {}\n",
    "\n",
    "with open('../../data/raw/yp_kimos-maui-lahaina_rws.json') as f:\n",
    "    json_data = json.loads(f.read())\n",
    "\n",
    "dataset = json_normalize(json_data['reviews'])\n",
    "dataset.head()\n",
    "\n",
    "lines = dataset.iloc[:,2].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_line(line):\n",
    "    line_arr = [x.lower() for x in line.split()]\n",
    "    return(' '.join(line_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_line(line):\n",
    "    return(line.replace('[^\\w\\s]',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_line(line):\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    line = ' '.join(x for x in line.split() if x not in stopwords_list)\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(lines)).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "\n",
    "def freqwords_line(line):\n",
    "    line = \" \".join(x for x in line.split() if x not in freq)\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = pd.Series(' '.join(lines)).value_counts()[-10:]\n",
    "\n",
    "def rarewords_line(line):\n",
    "    line = \" \".join(x for x in line.split() if x not in rare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck_line(line):\n",
    "    return(str(TextBlob(line).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_line(line):\n",
    "    return(\" \".join(TextBlob(str(line)).words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()\n",
    "def stemming_line(line):\n",
    "    line = \" \".join([st.stem(word) for word in line.split()])\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemnatize_line(line):\n",
    "    line = \" \".join([Word(word).lemmatize() for word in line.split()])\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Implementation on sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# tokenize\n",
    "lines_arr = []\n",
    "for line in lines:\n",
    "    # lower\n",
    "    line = lower_line(line)\n",
    "    # punctuation\n",
    "    line = punctuation_line(line)\n",
    "    # stopwords\n",
    "    line = stopwords_line(line)\n",
    "    # freq\n",
    "    line = freqwords_line(line)\n",
    "    # rare\n",
    "    # line = rarewords_line(line)\n",
    "    # spelling\n",
    "    # line = spelling_line(line)\n",
    "    # tokenize\n",
    "    line = tokenize_line(line)\n",
    "    # stemming\n",
    "    line = stemming_line(line)\n",
    "    # lemmatization\n",
    "    line = lemnatize_line(line)\n",
    "    \n",
    "    lines_arr.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This place was on my \"Must Do Maui\" list and it did not disappoint. You must come here for the view (we made a reservation and had mentioned we were celebrating our honeymoon, so we got a prime table on the second floor, by all the tiki torches), which is one of the best in the area. The service is worth it as well. You are greeted and served with such genuine warmth and hospitality, you will feel like you're among friends.\n",
      "\n",
      "Their menu is really diverse - lots of local favorites with a big fish menu, daily specials, and even prime rib and steak options. So there is something for everyone. Our server was quick to recommend suggestions and drink pairings, which really helped us navigate our experience. If you're in Maui, this is a great place for date night or dinner with family and friends. Check it out!\n",
      "--------\n",
      "place must maui list disappoint must come view we made reserv mention celebr honeymoon got prime tabl second floor tiki torch one best area servic worth well greet serv genuin warmth hospit feel like among friend menu realli diver lot local favorit big fish menu daili special even prime rib steak option someth everyon server quick recommend suggest drink pair realli help u navig experi maui great place date night dinner famili friend check out\n"
     ]
    }
   ],
   "source": [
    "print(lines[2])\n",
    "print('--------')\n",
    "print(lines_arr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
