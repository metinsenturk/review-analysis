{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text pre-processing\n",
    "\n",
    "The following basic preprocessing will be examined in this notebook. A similar example for this study can be found at [Analytics Vidhya's Website](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/).\n",
    "\n",
    "- Lower casing\n",
    "- Punctuation removal\n",
    "- Stopwords removal\n",
    "- Frequent words removal\n",
    "- Rare words removal\n",
    "- Spelling correction\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from nltk.stem import PorterStemmer\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "except:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imports\n",
    "json_data = {}\n",
    "\n",
    "with open('../../data/raw/yp_keokis-paradise-koloa_rws.json') as f:\n",
    "    json_data = json.loads(f.read())\n",
    "\n",
    "dataset = json_normalize(json_data['reviews'])\n",
    "dataset.head()\n",
    "\n",
    "lines = dataset.iloc[:,2].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_line(line):\n",
    "    line_arr = [x.lower() for x in line.split()]\n",
    "    return(' '.join(line_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_line(line):\n",
    "    return(line.replace('[^\\w\\s]',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_line(line):\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    line = ' '.join(x for x in line.split() if x not in stopwords_list)\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(lines)).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "\n",
    "def freqwords_line(line):\n",
    "    line = \" \".join(x for x in line.split() if x not in freq)\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = pd.Series(' '.join(lines)).value_counts()[-10:]\n",
    "\n",
    "def rarewords_line(line):\n",
    "    line = \" \".join(x for x in line.split() if x not in rare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck_line(line):\n",
    "    return(str(TextBlob(line).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_line(line):\n",
    "    return(\" \".join(TextBlob(str(line)).words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()\n",
    "def stemming_line(line):\n",
    "    line = \" \".join([st.stem(word) for word in line.split()])\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemnatize_line(line):\n",
    "    line = \" \".join([Word(word).lemmatize() for word in line.split()])\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Implementation on sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# tokenize\n",
    "lines_arr = []\n",
    "for line in lines:\n",
    "    # lower\n",
    "    line = lower_line(line)\n",
    "    # punctuation\n",
    "    line = punctuation_line(line)\n",
    "    # stopwords\n",
    "    line = stopwords_line(line)\n",
    "    # freq\n",
    "    line = freqwords_line(line)\n",
    "    # rare\n",
    "    # line = rarewords_line(line)\n",
    "    # spelling\n",
    "    # line = spelling_line(line)\n",
    "    # tokenize\n",
    "    line = tokenize_line(line)\n",
    "    # stemming\n",
    "    line = stemming_line(line)\n",
    "    # lemmatization\n",
    "    line = lemnatize_line(line)\n",
    "    \n",
    "    lines_arr.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is located on the beach side of Waikiki and it has an open setting where you can view the beach from the dining room.  We came here for brunch after a family photo shoot, you know, since we were in the area and they take reservations, makes life easier!\n",
      "\n",
      "Crab & Macadamia Nut Wontons - I mean come on, it's not from Hawaii if you don't have macadamia nuts in them!  Just kidding, we are in Waikiki after all, why not get the full island style food, by using what we grow!  I make crab wontons once in a blue moon, and it's a great way to make it \"Hawaii\" is to add macadamia nuts instead of water chestnuts, it gives a nice crunch and texture to the crab mixture and it goes well together. Can't go wrong with these for an appetizer, if you like crab.\n",
      "\n",
      "Fish & Chips - only because they had malt vinegar, I ordered this!  The fish pieces were a good size.  Moist on the inside meaning it wasn't dry and over-cooked.  Tartar sauce and malt vinegar, I'm in fish & chips heaven!  Potato wedges left with the skin on were good, not over-cooked either.\n",
      "\n",
      "Strawberry Mochi Waffles - breakfast for dessert!  So this was yummy!  The waffles were moist because well, it's mochi.  It was light in the sweetness, which was great!  I don't like when it's too sweet anyway.  The waffles were super soft and delicious.\n",
      "\n",
      "I'd definitely one back and either eat what I had or try something different, since what I had so far were really good.\n",
      "--------\n",
      "locat beach side waikiki open set view beach dine room came brunch famili photo shoot know sinc area take reserv make life easier crab macadamia nut wonton mean come on hawaii macadamia nut them kid waikiki all get full island style food use grow make crab wonton blue moon great way make hawaii add macadamia nut instead water chestnut give nice crunch textur crab mixtur goe well togeth ca n't go wrong appet like crab fish chip malt vinegar order thi fish piec good size moist insid mean dri over-cook tartar sauc malt vinegar i 'm fish chip heaven potato wedg left skin good over-cook either strawberri mochi waffl breakfast dessert yummi waffl moist well mochi light sweet great like sweet anyway waffl super soft delici i 'd definit one back either eat tri someth differ sinc far realli good\n"
     ]
    }
   ],
   "source": [
    "print(lines[2])\n",
    "print('--------')\n",
    "print(lines_arr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
