{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"../../data/raw/yp_competitors_rws.csv\"\n",
    "file_path = \"../../data/processed/yp_kimos-maui-lahaina_rws.csv\"\n",
    "df = pd.read_csv(file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "from itertools import chain\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk import FreqDist\n",
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Kimo's never disappoints.  We come here once o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I was there the first week of October and Firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This place was on my \"Must Do Maui\" list and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Monday night dinner here and they quickly sat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Nice view and amazing cocktails. They are loca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status                                            reviews\n",
       "0       1  Kimo's never disappoints.  We come here once o...\n",
       "1       0  I was there the first week of October and Firs...\n",
       "2       1  This place was on my \"Must Do Maui\" list and i...\n",
       "3       0  Monday night dinner here and they quickly sat ...\n",
       "4       1  Nice view and amazing cocktails. They are loca..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent_tokens'] = df.reviews.apply(lambda x: sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_tokens'] = df.sent_tokens.apply(\n",
    "    lambda x: [w\n",
    "               for s in x\n",
    "               for w in word_tokenize(s) if w.isalpha()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tag'] = df.word_tokens.apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = chain(*df.word_tokens)\n",
    "tokens = [w for w in tokens if w.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113970"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "- lowercased\n",
    "- punctuation removed\n",
    "- stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(w.lower() for w in tokens))\n",
    "tokens_freq = FreqDist(w.lower() for w in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7239"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7239"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06351671492498026"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexicality score\n",
    "len(vocab) / len(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', 1716), ('food', 1680), ('good', 1649), ('fish', 1531), ('service', 1105), ('view', 1056), ('kimo', 968), ('pie', 936), ('hula', 867), ('place', 860), ('maui', 793), ('dinner', 746), ('us', 686), ('would', 646), ('ordered', 603), ('get', 599), ('really', 593), ('one', 581), ('time', 575), ('back', 573)]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "vocab2 = [porter.stem(lemmatizer.lemmatize(w)) for i, w in enumerate(vocab)]\n",
    "# vocab2 = [lemmatizer.lemmatize(porter.stem(w)) for i, w in enumerate(vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2_freq = FreqDist(vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7239"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5226"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab2_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('manag', 8), ('consist', 7), ('season', 7), ('care', 6), ('celebr', 6), ('enjoy', 6), ('indulg', 6), ('person', 6), ('prepar', 6), ('recommend', 6), ('review', 6), ('select', 6), ('suggest', 6), ('travel', 6), ('arriv', 5), ('compar', 5), ('compens', 5), ('disappoint', 5), ('except', 5), ('expect', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(vocab2_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to each document in corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['norm_tokens'] = df.word_tokens.apply(\n",
    "    lambda x: [porter.stem(lemmatizer.lemmatize(w.lower()))\n",
    "                                  for w in x if w.lower() not in stopwords.words('english')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>reviews</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>norm_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Kimo's never disappoints.  We come here once o...</td>\n",
       "      <td>[Kimo's never disappoints., We come here once ...</td>\n",
       "      <td>[Kimo, never, disappoints, We, come, here, onc...</td>\n",
       "      <td>[(Kimo, NNP), (never, RB), (disappoints, VBZ),...</td>\n",
       "      <td>[kimo, never, disappoint, come, everi, trip, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I was there the first week of October and Firs...</td>\n",
       "      <td>[I was there the first week of October and Fir...</td>\n",
       "      <td>[I, was, there, the, first, week, of, October,...</td>\n",
       "      <td>[(I, PRP), (was, VBD), (there, RB), (the, DT),...</td>\n",
       "      <td>[first, week, octob, first, think, staff, wond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This place was on my \"Must Do Maui\" list and i...</td>\n",
       "      <td>[This place was on my \"Must Do Maui\" list and ...</td>\n",
       "      <td>[This, place, was, on, my, Must, Do, Maui, lis...</td>\n",
       "      <td>[(This, DT), (place, NN), (was, VBD), (on, IN)...</td>\n",
       "      <td>[place, must, maui, list, disappoint, must, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Monday night dinner here and they quickly sat ...</td>\n",
       "      <td>[Monday night dinner here and they quickly sat...</td>\n",
       "      <td>[Monday, night, dinner, here, and, they, quick...</td>\n",
       "      <td>[(Monday, NNP), (night, NN), (dinner, NN), (he...</td>\n",
       "      <td>[monday, night, dinner, quickli, sat, group, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Nice view and amazing cocktails. They are loca...</td>\n",
       "      <td>[Nice view and amazing cocktails., They are lo...</td>\n",
       "      <td>[Nice, view, and, amazing, cocktails, They, ar...</td>\n",
       "      <td>[(Nice, NNP), (view, NN), (and, CC), (amazing,...</td>\n",
       "      <td>[nice, view, amaz, cocktail, locat, right, lah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status                                            reviews  \\\n",
       "0       1  Kimo's never disappoints.  We come here once o...   \n",
       "1       0  I was there the first week of October and Firs...   \n",
       "2       1  This place was on my \"Must Do Maui\" list and i...   \n",
       "3       0  Monday night dinner here and they quickly sat ...   \n",
       "4       1  Nice view and amazing cocktails. They are loca...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [Kimo's never disappoints., We come here once ...   \n",
       "1  [I was there the first week of October and Fir...   \n",
       "2  [This place was on my \"Must Do Maui\" list and ...   \n",
       "3  [Monday night dinner here and they quickly sat...   \n",
       "4  [Nice view and amazing cocktails., They are lo...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [Kimo, never, disappoints, We, come, here, onc...   \n",
       "1  [I, was, there, the, first, week, of, October,...   \n",
       "2  [This, place, was, on, my, Must, Do, Maui, lis...   \n",
       "3  [Monday, night, dinner, here, and, they, quick...   \n",
       "4  [Nice, view, and, amazing, cocktails, They, ar...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [(Kimo, NNP), (never, RB), (disappoints, VBZ),...   \n",
       "1  [(I, PRP), (was, VBD), (there, RB), (the, DT),...   \n",
       "2  [(This, DT), (place, NN), (was, VBD), (on, IN)...   \n",
       "3  [(Monday, NNP), (night, NN), (dinner, NN), (he...   \n",
       "4  [(Nice, NNP), (view, NN), (and, CC), (amazing,...   \n",
       "\n",
       "                                         norm_tokens  \n",
       "0  [kimo, never, disappoint, come, everi, trip, m...  \n",
       "1  [first, week, octob, first, think, staff, wond...  \n",
       "2  [place, must, maui, list, disappoint, must, co...  \n",
       "3  [monday, night, dinner, quickli, sat, group, u...  \n",
       "4  [nice, view, amaz, cocktail, locat, right, lah...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/processed/yp_kimos-maui-lahaina_rws_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print([1 if w in vocab else 0 for w in df.norm_tokens[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Kimo's never disappoints.  We come here once o...\n",
       "1    I was there the first week of October and Firs...\n",
       "2    This place was on my \"Must Do Maui\" list and i...\n",
       "3    Monday night dinner here and they quickly sat ...\n",
       "4    Nice view and amazing cocktails. They are loca...\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: x.reviews, axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.word_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "df.head()\n",
    "\n",
    "documents = df.loc[:, ['norm_tokens', 'status']].values\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(vocab2_freq)#[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(rude) = True                0 : 1      =     58.2 : 1.0\n",
      "          contains(poor) = True                0 : 1      =     43.9 : 1.0\n",
      "        contains(switch) = True                0 : 1      =     41.6 : 1.0\n",
      "       contains(mediocr) = True                0 : 1      =     32.1 : 1.0\n",
      "          contains(wast) = True                0 : 1      =     29.2 : 1.0\n",
      "          contains(flag) = True                0 : 1      =     22.6 : 1.0\n",
      "         contains(stale) = True                0 : 1      =     22.6 : 1.0\n",
      "        contains(explan) = True                0 : 1      =     20.2 : 1.0\n",
      "         contains(empti) = True                0 : 1      =     18.8 : 1.0\n",
      "         contains(knife) = True                0 : 1      =     17.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Show the most important features as interpreted by Naive Bayes\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
