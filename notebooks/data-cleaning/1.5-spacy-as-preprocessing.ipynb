{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"../../data/processed/hi_rws_0001_0256_descriptive.csv\"\n",
    "df = pd.read_csv(file_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Preprocessing Steps\n",
    "\n",
    "1. Sent Tokens\n",
    "2. Word Tokens\n",
    "3. Remove punctuations \n",
    "4. Remove stopwords (NLTK List)\n",
    "5. Lower words\n",
    "6. Lemmatize\n",
    "7. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 21 columns):\n",
      "alias               1000 non-null object\n",
      "ratingValue         1000 non-null int64\n",
      "dataPublished       1000 non-null object\n",
      "description         1000 non-null object\n",
      "author              1000 non-null object\n",
      "sentiment           1000 non-null int64\n",
      "word_count          1000 non-null int64\n",
      "sent_count          1000 non-null int64\n",
      "chr_count           1000 non-null int64\n",
      "avg_word_len        1000 non-null float64\n",
      "avg_sent_len        1000 non-null float64\n",
      "num_of_stopwords    1000 non-null int64\n",
      "num_of_modals       1000 non-null int64\n",
      "hashtags            1000 non-null int64\n",
      "mentions            1000 non-null int64\n",
      "numerics            1000 non-null int64\n",
      "uppercase_cnt       1000 non-null int64\n",
      "punctuation_cnt     1000 non-null int64\n",
      "vocab_cnt           1000 non-null int64\n",
      "ratio_lexical       1000 non-null float64\n",
      "ratio_content       1000 non-null float64\n",
      "dtypes: float64(4), int64(13), object(4)\n",
      "memory usage: 164.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x11775cf98>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['ner'])\n",
    "print(spacy.__version__)\n",
    "nlp.pipe_names\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Testing with a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_py_tokens', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_disk', 'get_extension', 'get_lca_matrix', 'has_extension', 'has_vector', 'is_parsed', 'is_sentenced', 'is_tagged', 'mem', 'merge', 'noun_chunks', 'noun_chunks_iterator', 'print_tree', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_extension', 'similarity', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_disk', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nlp(df.description[5])\n",
    "sent = [i for i in text.sents][0]\n",
    "print(dir(text))\n",
    "type(text[0].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution one that I dont like\n",
    "texts = []\n",
    "for sent_token in text.sents:\n",
    "\n",
    "    for token in text:        \n",
    "        if token in sent_token:\n",
    "            # do cleaning in here\n",
    "            texts.append(clean_up(sent_token.text))\n",
    "[tuple(i) for i in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solution that is fair enough\n",
    "texts = []\n",
    "sent_current = \"\"\n",
    "for token in text:     \n",
    "    # check for tokens current sent\n",
    "    if sent_current == token.sent.text: \n",
    "        # add same sent tokens to the sent list\n",
    "        token_clean = token_clean_up(token)\n",
    "        if token_clean is not None:\n",
    "            sent.append(token_clean)\n",
    "            print(sent)\n",
    "    else:         \n",
    "        # add it to texts, if it is not initially\n",
    "        if sent_current != \"\":\n",
    "            texts.append(sent)\n",
    "        # update current sent index\n",
    "        sent_current = token.sent.text\n",
    "        # create sent list and add first token\n",
    "        sent = []\n",
    "        token_clean = token_clean_up(token)\n",
    "        if token_clean is not None:\n",
    "            sent.append(token_clean)\n",
    "texts.append(sent)\n",
    "print(len(list(text.sents)), len(texts))\n",
    "# [print(i) for i in text.sents]\n",
    "# [tuple(i) for i in texts]\n",
    "# [print(i, '\\n\\n', y) for i,y in zip(text.sents, texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunk_list = []\n",
    "for token in text.noun_chunks:\n",
    "    if token.is_stop == False and token.is_alpha and len(token) > 3 and token.pos_ not in removal:\n",
    "        noun_chunk_list.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.orth_ for i in text][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing in Review DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_clean_up(token):\n",
    "    \"\"\" token cleanup. Return clean token or None. \"\"\"\n",
    "    removal=['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "    if token.is_stop == False and token.is_alpha and len(token)>3 and token.pos_ not in removal:\n",
    "        lemma = token.lemma_\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(text):  \n",
    "    \"\"\" clean up tokens by documents \"\"\"\n",
    "    removal = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "    doc = nlp(text)\n",
    "    text_out = []    \n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.is_alpha and len(token)>3 and token.pos_ not in removal:\n",
    "            lemma = token.lemma_\n",
    "            text_out.append(lemma)\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up2(text, clean_up=False):\n",
    "    \"\"\" clean up tokens by sents in documents \"\"\"\n",
    "    removal=['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    texts = []\n",
    "    sent_current = \"\"\n",
    "    for token in doc:    \n",
    "        # check for tokens current sent\n",
    "        if sent_current != token.sent.text:\n",
    "            # add it to texts, if it is not initially\n",
    "            if sent_current != \"\":\n",
    "                if len(sent) > 0:\n",
    "                    texts.append(sent)\n",
    "            # update current sent index\n",
    "            sent_current = token.sent.text\n",
    "            # create sent list and add first token\n",
    "            sent = []\n",
    "            if clean_up:\n",
    "                token_clean = token_clean_up(token)\n",
    "                if token_clean is not None:\n",
    "                    sent.append(token_clean)\n",
    "            else:\n",
    "                sent.append(token)\n",
    "        else:        \n",
    "            # add same sent tokens to the sent list\n",
    "            if clean_up:\n",
    "                token_clean = token_clean_up(token)\n",
    "                if token_clean is not None:\n",
    "                    sent.append(token_clean)\n",
    "            else:\n",
    "                sent.append(token)\n",
    "    # add the last sentence to the list\n",
    "    texts.append(sent)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.description.apply(lambda x: clean_up2(x, True))\n",
    "type(df_test[0][0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 8088\n"
     ]
    }
   ],
   "source": [
    "print(len(df.description), sum(len(i) for i in df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3210"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with gensim\n",
    "from itertools import chain\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(chain(*df_test))\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing New Feature Creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I stumbled across this great restaurant overlooking the ocean for lunch during my vacation to Maui. I did not have high expectations for this place, but boy did it blow me out of the water. \n",
      "\n",
      "The fish and chips is some of the best I've ever had (and I've had lots, including from London). I highly recommend it. Also, the turkey bacon sandwich was SO good. \n",
      "\n",
      "In terms of drinks, I highly recommend the Pacific Paradise drink! So delicious and tropical! I also really enjoyed the Lahaina Lemonade. \n",
      "\n",
      "Service was really great! I wish I remembered the waitresses name because she was truly awesome and recommend the best stuff. She was blonde and had cute sunglasses.\n",
      "I\n",
      "I\n",
      "this great restaurant\n",
      "restaurant\n",
      "the ocean\n",
      "ocean\n",
      "lunch\n",
      "lunch\n",
      "my vacation\n",
      "vacation\n",
      "Maui\n",
      "Maui\n",
      "I\n",
      "I\n",
      "high expectations\n",
      "expectations\n",
      "this place\n",
      "place\n",
      "it\n",
      "it\n",
      "me\n",
      "me\n",
      "the water\n",
      "water\n",
      "The fish\n",
      "fish\n",
      "chips\n",
      "chips\n",
      "I\n",
      "I\n",
      "I\n",
      "I\n",
      "lots\n",
      "lots\n",
      "London\n",
      "London\n",
      "I\n",
      "I\n",
      "it\n",
      "it\n",
      "the turkey bacon sandwich\n",
      "sandwich\n",
      "terms\n",
      "terms\n",
      "drinks\n",
      "drinks\n",
      "I\n",
      "I\n",
      "I\n",
      "I\n",
      "the Lahaina Lemonade. \n",
      "\n",
      "Service\n",
      "Service\n",
      "I\n",
      "I\n",
      "I\n",
      "I\n",
      "the waitresses\n",
      "waitresses\n",
      "she\n",
      "she\n",
      "the best stuff\n",
      "stuff\n",
      "She\n",
      "She\n",
      "cute sunglasses\n",
      "sunglasses\n",
      "Excellent view on the ocean at sunset.\n",
      "Excellent food. We had the fresh fish : coconut for me and the yuzu for my husband. We loved it!\n",
      "Waitress are super nice.\n",
      "Excellent view\n",
      "view\n",
      "the ocean\n",
      "ocean\n",
      "sunset\n",
      "sunset\n",
      "Excellent food\n",
      "food\n",
      "We\n",
      "We\n",
      "the fresh fish\n",
      "fish\n",
      "coconut\n",
      "coconut\n",
      "me\n",
      "me\n",
      "the yuzu\n",
      "yuzu\n",
      "my husband\n",
      "husband\n",
      "We\n",
      "We\n",
      "it\n",
      "it\n",
      "Waitress\n",
      "Waitress\n",
      "This place was not what the reviews portrayed at all. For starters, we were walked up stairs and wanted to sit on the balcony (to catch the sun set). We were told that they had to ask if we can sit there (it was a table that was just being cleaned after a couple had just left). As the we waited, another server walked right by us and sat another couple there who had just walked in...I did not like that at all...but we stayed. We ended up waiting for another table in the same place and after about \n",
      "40 min, our table was finally ready. \n",
      "\n",
      "The food was just okay...the vegetation option that I had was a ravioli dish with tofu and veggies. The tofu, although it looked like a nice grilled piece of salmon, had ZERO flavor!! I've had tofu plenty of times and you can actually get it to pick up any flavor of food you make it in. Not true with this dish. My husband has a fish dish with rice and veggies (and a side of sweet potato and Brussels hash). Unfortunately, the hash was the only thing that he liked. We've been coming to Maui now for almost 8 years and we like to try new restaurants (local, casual, and more pricier settings). \n",
      "This was our first time trying Kimo's and unfortunately, it will be our last. \n",
      "\n",
      "Sorry..l\n",
      "OB\n",
      "This place\n",
      "place\n",
      "what\n",
      "what\n",
      "the reviews\n",
      "reviews\n",
      "starters\n",
      "starters\n",
      "we\n",
      "we\n",
      "stairs\n",
      "stairs\n",
      "the balcony\n",
      "balcony\n",
      "We\n",
      "We\n",
      "they\n",
      "they\n",
      "we\n",
      "we\n",
      "it\n",
      "it\n",
      "a table\n",
      "table\n",
      "a couple\n",
      "couple\n",
      "the we\n",
      "we\n",
      "another server\n",
      "server\n",
      "us\n",
      "us\n",
      "another couple\n",
      "couple\n",
      "who\n",
      "who\n",
      "I\n",
      "I\n",
      "we\n",
      "we\n",
      "We\n",
      "We\n",
      "another table\n",
      "table\n",
      "the same place\n",
      "place\n",
      "40 min\n",
      "min\n",
      "our table\n",
      "table\n",
      "The food\n",
      "food\n",
      "the vegetation option\n",
      "option\n",
      "I\n",
      "I\n",
      "a ravioli dish\n",
      "dish\n",
      "tofu\n",
      "tofu\n",
      "veggies\n",
      "veggies\n",
      "The tofu\n",
      "tofu\n",
      "it\n",
      "it\n",
      "a nice grilled piece\n",
      "piece\n",
      "salmon\n",
      "salmon\n",
      "ZERO flavor\n",
      "flavor\n",
      "I\n",
      "I\n",
      "tofu\n",
      "tofu\n",
      "plenty\n",
      "plenty\n",
      "times\n",
      "times\n",
      "you\n",
      "you\n",
      "it\n",
      "it\n",
      "any flavor\n",
      "flavor\n",
      "food\n",
      "food\n",
      "you\n",
      "you\n",
      "it\n",
      "it\n",
      "this dish\n",
      "dish\n",
      "My husband\n",
      "husband\n",
      "a fish dish\n",
      "dish\n",
      "rice\n",
      "rice\n",
      "veggies\n",
      "veggies\n",
      "a side\n",
      "side\n",
      "sweet potato\n",
      "potato\n",
      "Brussels hash\n",
      "hash\n",
      "the hash\n",
      "hash\n",
      "the only thing\n",
      "thing\n",
      "he\n",
      "he\n",
      "We\n",
      "We\n",
      "Maui\n",
      "Maui\n",
      "almost 8 years\n",
      "years\n",
      "we\n",
      "we\n",
      "new restaurants\n",
      "restaurants\n",
      "local, casual, and more pricier settings\n",
      "settings\n",
      "our first time\n",
      "time\n",
      "Kimo\n",
      "Kimo\n",
      "it\n",
      "it\n",
      "l\n",
      "l\n",
      "OB\n",
      "OB\n",
      "We were excited to repeat our Keoki's (in Kauai) lovefest since this is also a part of their chain restaurants. We were very disappointed. \n",
      "\n",
      "This is in the center of Lahaina so parking was difficult to find during dinner time. We ended up having to pay $15 for 4hrs at a parking lot a block away. \n",
      "\n",
      "When we arrived, it was packed. We thought that would be a good sign. Since it's night time, there wasn't a view to enjoy but the decor of the place was very nice. A little kitschy boat beach theme but nice. Our server was friendly and all the waitstaff were also friendly. Our waters were always filled promptly. It was extremely loud and didn't feel very intimate at all. If anything, it felt like a sports bar without all the TVs.\n",
      "\n",
      "We ordered a ginger mojito and a mango colada. The mojito was not good at all and we regretted ordering it. But that mango colada was delicious. We could have ordered several more of those! \n",
      "\n",
      "The carrot muffins are the same as Keoki's and delicious. \n",
      "\n",
      "The calamari was lackluster and we wish we hadn't ordered it. The ones at Keoki's are way better. \n",
      "\n",
      "I love the prime rib at Keoki's and was looking forward to the same delicious prime rib at Kimo's but was sorely disappointed. The prime rib had no flavor and was ridiculously salty. After about 5 bites, I gave up on it. There was also a huge hunk of fat which counts towards the 14oz order. The side of Maui vegetables was also overly salty. My partner got the ribs which were ok at best. Our food was also room temperature when it arrived which really ticked me off because it had clearly been sitting there. I considered sending back my prime rib but if it's salty, then any other piece would have been salty too. We either had bad luck with our chef who was salt friendly or something was amiss with our food. The table next to us just have had the opposite with their dishes because one of them was dousing his dish heavily with soy sauce.\n",
      "\n",
      "Kimo's is nothing like Keoki's. If we could undo this meal, we would. I loved the carrot muffin and mango colada but that's not enough to bring me back. We were in and out in an hour. I would have stayed for more mango coladas but our food was so disappointing I didn't want to stay any longer.\n",
      "We\n",
      "We\n",
      "our Keoki\n",
      "Keoki\n",
      "Kauai\n",
      "Kauai\n",
      "a part\n",
      "part\n",
      "their chain restaurants\n",
      "restaurants\n",
      "We\n",
      "We\n",
      "the center\n",
      "center\n",
      "Lahaina\n",
      "Lahaina\n",
      "parking\n",
      "parking\n",
      "dinner time\n",
      "time\n",
      "We\n",
      "We\n",
      "a parking lot\n",
      "lot\n",
      "we\n",
      "we\n",
      "it\n",
      "it\n",
      "We\n",
      "We\n",
      "a good sign\n",
      "sign\n",
      "it\n",
      "it\n",
      "night time\n",
      "time\n",
      "a view\n",
      "view\n",
      "the decor\n",
      "decor\n",
      "the place\n",
      "place\n",
      "A little kitschy boat beach theme\n",
      "theme\n",
      "Our server\n",
      "server\n",
      "all the waitstaff\n",
      "waitstaff\n",
      "Our waters\n",
      "waters\n",
      "It\n",
      "It\n",
      "it\n",
      "it\n",
      "a sports bar\n",
      "bar\n",
      "all the TVs\n",
      "TVs\n",
      "We\n",
      "We\n",
      "a ginger mojito\n",
      "mojito\n",
      "a mango colada\n",
      "colada\n",
      "The mojito\n",
      "mojito\n",
      "we\n",
      "we\n",
      "it\n",
      "it\n",
      "mango colada\n",
      "colada\n",
      "We\n",
      "We\n",
      "The carrot muffins\n",
      "muffins\n",
      "Keoki\n",
      "Keoki\n",
      "The calamari\n",
      "calamari\n",
      "we\n",
      "we\n",
      "we\n",
      "we\n",
      "it\n",
      "it\n",
      "The ones\n",
      "ones\n",
      "Keoki\n",
      "Keoki\n",
      "I\n",
      "I\n",
      "the prime rib\n",
      "rib\n",
      "Keoki\n",
      "Keoki\n",
      "the same delicious prime rib\n",
      "rib\n",
      "Kimo\n",
      "Kimo\n",
      "The prime rib\n",
      "rib\n",
      "no flavor\n",
      "flavor\n",
      "about 5 bites\n",
      "bites\n",
      "I\n",
      "I\n",
      "it\n",
      "it\n",
      "a huge hunk\n",
      "hunk\n",
      "fat\n",
      "fat\n",
      "the 14oz order\n",
      "order\n",
      "The side\n",
      "side\n",
      "Maui vegetables\n",
      "vegetables\n",
      "My partner\n",
      "partner\n",
      "the ribs\n",
      "ribs\n",
      "Our food\n",
      "food\n",
      "room temperature\n",
      "temperature\n",
      "it\n",
      "it\n",
      "me\n",
      "me\n",
      "it\n",
      "it\n",
      "I\n",
      "I\n",
      "my prime rib\n",
      "rib\n",
      "it\n",
      "it\n",
      "any other piece\n",
      "piece\n",
      "We\n",
      "We\n",
      "bad luck\n",
      "luck\n",
      "our chef\n",
      "chef\n",
      "who\n",
      "who\n",
      "something\n",
      "something\n",
      "our food\n",
      "food\n",
      "The table\n",
      "table\n",
      "us\n",
      "us\n",
      "the opposite\n",
      "opposite\n",
      "their dishes\n",
      "dishes\n",
      "them\n",
      "them\n",
      "his dish\n",
      "dish\n",
      "soy sauce\n",
      "sauce\n",
      "Kimo\n",
      "Kimo\n",
      "nothing\n",
      "nothing\n",
      "Keoki\n",
      "Keoki\n",
      "we\n",
      "we\n",
      "this meal\n",
      "meal\n",
      "we\n",
      "we\n",
      "I\n",
      "I\n",
      "the carrot muffin\n",
      "muffin\n",
      "mango colada\n",
      "colada\n",
      "me\n",
      "me\n",
      "We\n",
      "We\n",
      "an hour\n",
      "hour\n",
      "I\n",
      "I\n",
      "more mango coladas\n",
      "coladas\n",
      "our food\n",
      "food\n",
      "I\n",
      "I\n",
      "If you're looking for a tourist spot, this is it. Unfortunately I couldn't tell that from other reviews. \n",
      "\n",
      "Food: Main dish was tasty but the sides were simply steamed without any seasoning or pizaz. The hula pie which was highly recommended was just a huge slice of ice cream, way too much to eat and way to expensive for what it was. One of the biggest red flags was that a three course meal took 45 min from being seated to paying the check. This made us worry that the food was all pre-prepared and not made to order.\n",
      "\n",
      "Service: wait staff was incredibly kind, funny, and attentive. \n",
      "\n",
      "Price: expensive meal given the quality. Ended up spending just over $60 a person. \n",
      "\n",
      "Atmosphere: nice ocean and sunset views\n",
      "\n",
      "Tl:dr If you're a bit of a food snob, this joint isn't for you.\n",
      "you\n",
      "you\n",
      "a tourist spot\n",
      "spot\n",
      "it\n",
      "it\n",
      "I\n",
      "I\n",
      "other reviews\n",
      "reviews\n",
      "Food\n",
      "Food\n",
      "Main dish\n",
      "dish\n",
      "the sides\n",
      "sides\n",
      "any seasoning\n",
      "seasoning\n",
      "pizaz\n",
      "pizaz\n",
      "The hula pie\n",
      "pie\n",
      "just a huge slice\n",
      "slice\n",
      "ice cream\n",
      "cream\n",
      "what\n",
      "what\n",
      "it\n",
      "it\n",
      "the biggest red flags\n",
      "flags\n",
      "a three course meal\n",
      "meal\n",
      "45 min\n",
      "min\n",
      "the check\n",
      "check\n",
      "us\n",
      "us\n",
      "the food\n",
      "food\n",
      "Service\n",
      "Service\n",
      "wait staff\n",
      "staff\n",
      "Price\n",
      "Price\n",
      "expensive meal\n",
      "meal\n",
      "the quality\n",
      "quality\n",
      "Atmosphere\n",
      "Atmosphere\n",
      "nice ocean and sunset views\n",
      "views\n",
      "dr\n",
      "dr\n",
      "you\n",
      "you\n",
      "a bit\n",
      "bit\n",
      "a food snob\n",
      "snob\n",
      "this joint\n",
      "joint\n",
      "you\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
    "texts = df.description[:5].apply(lambda x: nlp(x))\n",
    "for doc in texts:\n",
    "    print(doc)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # print(chunk.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA]))\n",
    "        print(chunk)\n",
    "        print(chunk.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_notnoun(phrase):\n",
    "    doc = nlp(phrase) # create spacy object\n",
    "    token_not_noun = []\n",
    "    notnoun_noun_list = []\n",
    "\n",
    "    for item in doc:\n",
    "        if item.pos_ != \"NOUN\": # separate nouns and not nouns\n",
    "            #if item.is_stop == False and item.is_oov == False and item.is_alpha:\n",
    "            token_clean = token_clean_up(item)\n",
    "            if token_clean is not None:\n",
    "                token_not_noun.append(token_clean)\n",
    "        if item.pos_ == \"NOUN\":\n",
    "            noun = item.text\n",
    "\n",
    "    for notnoun in token_not_noun:\n",
    "        notnoun_noun_list.append(notnoun + \" \" + noun)\n",
    "\n",
    "    return notnoun_noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excellent view on the ocean at sunset.\n",
      "Excellent food. We had the fresh fish : coconut for me and the yuzu for my husband. We loved it!\n",
      "Waitress are super nice.\n",
      "['excellent husband', 'excellent husband', 'fresh husband', 'love husband', 'waitress husband', 'nice husband']\n"
     ]
    }
   ],
   "source": [
    "print(df.description[1])\n",
    "print(noun_notnoun(df.description[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
