{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"../../data/processed/hi_rws_0001_0256_descriptive.csv\"\n",
    "df = pd.read_csv(file_path, nrows=100, memory_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Preprocessing Steps\n",
    "\n",
    "1. Sent Tokens\n",
    "2. Word Tokens\n",
    "3. Remove punctuations \n",
    "4. Remove stopwords (NLTK List)\n",
    "5. Lower words\n",
    "6. Lemmatize\n",
    "7. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 21 columns):\n",
      "alias               100 non-null object\n",
      "ratingValue         100 non-null int64\n",
      "dataPublished       100 non-null object\n",
      "description         100 non-null object\n",
      "author              100 non-null object\n",
      "sentiment           100 non-null int64\n",
      "word_count          100 non-null int64\n",
      "sent_count          100 non-null int64\n",
      "chr_count           100 non-null int64\n",
      "avg_word_len        100 non-null float64\n",
      "avg_sent_len        100 non-null float64\n",
      "num_of_stopwords    100 non-null int64\n",
      "num_of_modals       100 non-null int64\n",
      "hashtags            100 non-null int64\n",
      "mentions            100 non-null int64\n",
      "numerics            100 non-null int64\n",
      "uppercase_cnt       100 non-null int64\n",
      "punctuation_cnt     100 non-null int64\n",
      "vocab_cnt           100 non-null int64\n",
      "ratio_lexical       100 non-null float64\n",
      "ratio_content       100 non-null float64\n",
      "dtypes: float64(4), int64(13), object(4)\n",
      "memory usage: 16.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x15c550588>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "print(spacy.__version__)\n",
    "nlp.pipe_names\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_py_tokens', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_disk', 'get_extension', 'get_lca_matrix', 'has_extension', 'has_vector', 'is_parsed', 'is_sentenced', 'is_tagged', 'mem', 'merge', 'noun_chunks', 'noun_chunks_iterator', 'print_tree', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_extension', 'similarity', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_disk', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nlp(df.description[5])\n",
    "sent = [i for i in text.sents][0]\n",
    "print(dir(text))\n",
    "type(text[0].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in text.noun_chunks]\n",
    "text[0].sent.text != 'I stumbled across this great my vacation to Maui.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution one that I dont like\n",
    "texts = []\n",
    "for sent_token in text.sents:\n",
    "\n",
    "    for token in text:        \n",
    "        if token in sent_token:\n",
    "            # do cleaning in here\n",
    "            texts.append(clean_up(sent_token.text))\n",
    "[tuple(i) for i in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'amazing']\n",
      "['what', 'amazing', 'restaurant']\n",
      "['what', 'amazing', 'restaurant', 'view']\n",
      "['what', 'amazing', 'restaurant', 'view', 'dinner']\n",
      "['din']\n",
      "['din', 'wave']\n",
      "['din', 'wave', 'crash']\n",
      "['din', 'wave', 'crash', 'nice']\n",
      "['din', 'wave', 'crash', 'nice', 'relaxing']\n",
      "['food']\n",
      "['food', 'good']\n",
      "['food', 'good', 'start']\n",
      "['food', 'good', 'start', 'avocado']\n",
      "['food', 'good', 'start', 'avocado', 'stack']\n",
      "['food', 'good', 'start', 'avocado', 'stack', 'disappoint']\n",
      "['order']\n",
      "['order', 'steak']\n",
      "['order', 'steak', 'lobster']\n",
      "['order', 'steak', 'lobster', 'notice']\n",
      "['order', 'steak', 'lobster', 'notice', 'lobster']\n",
      "['order', 'steak', 'lobster', 'notice', 'lobster', 'tail']\n",
      "['order', 'steak', 'lobster', 'notice', 'lobster', 'tail', 'seasoning']\n",
      "['order', 'steak', 'lobster', 'notice', 'lobster', 'tail', 'seasoning', 'come']\n",
      "['order', 'steak', 'lobster', 'notice', 'lobster', 'tail', 'seasoning', 'come', 'char']\n",
      "['order', 'steak', 'lobster', 'notice', 'lobster', 'tail', 'seasoning', 'come', 'char', 'great']\n",
      "['order', 'steak', 'lobster', 'notice', 'lobster', 'tail', 'seasoning', 'come', 'char', 'great', 'taste']\n",
      "['forget']\n",
      "['forget', 'hula']\n",
      "['pricey']\n",
      "['pricey', 'people']\n",
      "['pricey', 'people', 'average']\n",
      "['pricey', 'people', 'average', 'bill']\n",
      "['pricey', 'people', 'average', 'bill', 'come']\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "# solution that is fair enough\n",
    "texts = []\n",
    "sent_current = \"\"\n",
    "for token in text:     \n",
    "    # check for tokens current sent\n",
    "    if sent_current == token.sent.text: \n",
    "        # add same sent tokens to the sent list\n",
    "        token_clean = token_clean_up(token)\n",
    "        if token_clean is not None:\n",
    "            sent.append(token_clean)\n",
    "            print(sent)\n",
    "    else:         \n",
    "        # add it to texts, if it is not initially\n",
    "        if sent_current != \"\":\n",
    "            texts.append(sent)\n",
    "        # update current sent index\n",
    "        sent_current = token.sent.text\n",
    "        # create sent list and add first token\n",
    "        sent = []\n",
    "        token_clean = token_clean_up(token)\n",
    "        if token_clean is not None:\n",
    "            sent.append(token_clean)\n",
    "texts.append(sent)\n",
    "print(len(list(text.sents)), len(texts))\n",
    "# [print(i) for i in text.sents]\n",
    "# [tuple(i) for i in texts]\n",
    "# [print(i, '\\n\\n', y) for i,y in zip(text.sents, texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_clean_up(token):\n",
    "    \"\"\" token cleanup. Return clean token or None. \"\"\"\n",
    "    removal=['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "    if token.is_stop == False and token.is_alpha and len(token)>3 and token.pos_ not in removal:\n",
    "        lemma = token.lemma_\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(text):  \n",
    "    \"\"\" clean up tokens by documents \"\"\"\n",
    "    removal = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "    doc = nlp(text)\n",
    "    text_out = []    \n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.is_alpha and len(token)>3 and token.pos_ not in removal:\n",
    "            lemma = token.lemma_\n",
    "            text_out.append(lemma)\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up2(text, clean_up=False):\n",
    "    \"\"\" clean up tokens by sents in documents \"\"\"\n",
    "    removal=['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    texts = []\n",
    "    sent_current = \"\"\n",
    "    for token in doc:    \n",
    "        # check for tokens current sent\n",
    "        if sent_current != token.sent.text:\n",
    "            # add it to texts, if it is not initially\n",
    "            if sent_current != \"\":\n",
    "                texts.append(sent)\n",
    "            # update current sent index\n",
    "            sent_current = token.sent.text\n",
    "            # create sent list and add first token\n",
    "            sent = []\n",
    "            if clean_up:\n",
    "                token_clean = token_clean_up(token)\n",
    "                if token_clean is not None:\n",
    "                    sent.append(token_clean)\n",
    "            else:\n",
    "                sent.append(token)\n",
    "        else:        \n",
    "            # add same sent tokens to the sent list\n",
    "            if clean_up:\n",
    "                token_clean = token_clean_up(token)\n",
    "                if token_clean is not None:\n",
    "                    sent.append(token_clean)\n",
    "            else:\n",
    "                sent.append(token)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.description.apply(lambda x: clean_up2(x, True))\n",
    "type(df_test[0][0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1127"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with gensim\n",
    "from itertools import chain\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(chain(*df_test))\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
