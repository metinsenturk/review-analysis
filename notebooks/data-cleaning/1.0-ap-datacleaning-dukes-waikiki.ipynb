{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text pre-processing\n",
    "\n",
    "The following basic preprocessing will be examined in this notebook. A similar example for this study can be found at [Analytics Vidhya's Website](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/).\n",
    "\n",
    "- Lower casing\n",
    "- Punctuation removal\n",
    "- Stopwords removal\n",
    "- Frequent words removal\n",
    "- Rare words removal\n",
    "- Spelling correction\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from nltk.stem import PorterStemmer\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "except:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imports\n",
    "json_data = {}\n",
    "\n",
    "with open('../../data/raw/yp_dukes-waikiki-honolulu-2_rws.json') as f:\n",
    "    json_data = json.loads(f.read())\n",
    "\n",
    "dataset = json_normalize(json_data['reviews'])\n",
    "dataset.head()\n",
    "\n",
    "lines = dataset.iloc[:,2].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_line(line):\n",
    "    line_arr = [x.lower() for x in line.split()]\n",
    "    return(' '.join(line_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_line(line):\n",
    "    return(line.replace('[^\\w\\s]',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_line(line):\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    line = ' '.join(x for x in line.split() if x not in stopwords_list)\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(lines)).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "\n",
    "def freqwords_line(line):\n",
    "    line = \" \".join(x for x in line.split() if x not in freq)\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = pd.Series(' '.join(lines)).value_counts()[-10:]\n",
    "\n",
    "def rarewords_line(line):\n",
    "    line = \" \".join(x for x in line.split() if x not in rare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck_line(line):\n",
    "    return(str(TextBlob(line).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_line(line):\n",
    "    return(\" \".join(TextBlob(str(line)).words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()\n",
    "def stemming_line(line):\n",
    "    line = \" \".join([st.stem(word) for word in line.split()])\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemnatize_line(line):\n",
    "    line = \" \".join([Word(word).lemmatize() for word in line.split()])\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Implementation on sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# tokenize\n",
    "lines_arr = []\n",
    "for line in lines:\n",
    "    # lower\n",
    "    line = lower_line(line)\n",
    "    # punctuation\n",
    "    line = punctuation_line(line)\n",
    "    # stopwords\n",
    "    line = stopwords_line(line)\n",
    "    # freq\n",
    "    line = freqwords_line(line)\n",
    "    # rare\n",
    "    # line = rarewords_line(line)\n",
    "    # spelling\n",
    "    # line = spelling_line(line)\n",
    "    # tokenize\n",
    "    line = tokenize_line(line)\n",
    "    # stemming\n",
    "    line = stemming_line(line)\n",
    "    # lemmatization\n",
    "    line = lemnatize_line(line)\n",
    "    \n",
    "    lines_arr.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by on a weekday night and the place was absolutely packed with 50 minute wait times both for the dining area and beachside seating. Every after some rain/sprinkles which washed some people out from their tables, it still took them awhile to bus and refill tables. In addition the hostess on the beachside was out of wireless buzzers to call you, so we had to check back in every 10-15 minutes to finally get one and often he forgot who needed one next.\n",
      "\n",
      "Our server however was absolutely amazing, very friendly, attentive and provided excellent drink recommendations. Without a doubt try the coconut mojito if you're looking a refreshing, not overly sweet cocktail to enjoy the view with. \n",
      "\n",
      "Food however was quite disappointing, at least for my meal, which was the macadamia crusted chicken katsu. The chicken was over cooked and dry, as was the crust itself which lacked any macadamia nuts. Rice had definitely sat under a heat lamp for awhile as it had formed a hard outer crust.... and why in the world would you serve plain ketchup with kastu instead of properly made tonkatsu? Disgusting. If the food had not taken 25 minutes, I probably would have returned it back and asked for something else.\n",
      "\n",
      "My date however was very happy with her beet salad and had no complaints. The waiter was more than happy to fill her special order request of adding bacon to it as well. \n",
      "\n",
      "I agree with many others that this place is extremely overrated. If not for the excellent server, I would have rated the place 1-2 stars... choose elsewhere.\n",
      "--------\n",
      "stop weekday night place absolut pack 50 minut wait time dine area beachsid seat everi rain/sprinkl wash peopl tabl still took awhil bu refil tabl addit hostess beachsid wireless buzzer call you check back everi 10-15 minut final get one often forgot need one next server howev absolut amaz friendli attent provid excel drink recommend without doubt tri coconut mojito look refresh overli sweet cocktail enjoy view with food howev quit disappoint least meal macadamia crust chicken katsu chicken cook dri crust lack macadamia nut rice definit sat heat lamp awhil form hard outer crust world would serv plain ketchup kastu instead properli made tonkatsu disgust food taken 25 minut probabl would return back ask someth el date howev happi beet salad complaint waiter happi fill special order request ad bacon well agre mani other place extrem overr excel server would rate place 1-2 star choos elsewher\n"
     ]
    }
   ],
   "source": [
    "print(lines[2])\n",
    "print('--------')\n",
    "print(lines_arr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
