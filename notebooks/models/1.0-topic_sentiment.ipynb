{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 454043 entries, 0 to 454042\n",
      "Data columns (total 26 columns):\n",
      "alias               454043 non-null object\n",
      "ratingValue         454043 non-null int64\n",
      "dataPublished       454043 non-null object\n",
      "description         454043 non-null object\n",
      "author              454043 non-null object\n",
      "sentiment           454043 non-null int64\n",
      "word_count          454043 non-null int64\n",
      "sent_count          454043 non-null int64\n",
      "chr_count           454043 non-null int64\n",
      "avg_word_len        454043 non-null float64\n",
      "avg_sent_len        454043 non-null float64\n",
      "num_of_stopwords    454043 non-null int64\n",
      "num_of_modals       454043 non-null int64\n",
      "hashtags            454043 non-null int64\n",
      "mentions            454043 non-null int64\n",
      "numerics            454043 non-null int64\n",
      "uppercase_cnt       454043 non-null int64\n",
      "punctuation_cnt     454043 non-null int64\n",
      "vocab_cnt           454043 non-null int64\n",
      "ratio_lexical       454043 non-null float64\n",
      "ratio_content       454043 non-null float64\n",
      "sent_tokens         454043 non-null object\n",
      "word_tokens_doc     454043 non-null object\n",
      "norm_tokens_doc     454043 non-null object\n",
      "word_tokens         454043 non-null object\n",
      "norm_tokens         454043 non-null object\n",
      "dtypes: float64(4), int64(13), object(9)\n",
      "memory usage: 93.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('../../data/processed/yp_competitors_rws_0001_0256_basicfeatures.csv')\n",
    "df2 = pd.read_csv('../../data/processed/yp_competitors_rws_0001_0256_textfeatures.csv')\n",
    "df = df1.merge(df2, on=['alias', 'ratingValue', 'dataPublished', 'description', 'author'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fixes\n",
    "from itertools import chain\n",
    "import ast\n",
    "# csv list fix with : ast\n",
    "df.sent_tokens = df.sent_tokens.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "df.word_tokens_doc = df.word_tokens_doc.apply(lambda x: ast.literal_eval(x))\n",
    "df.norm_tokens_doc = df.norm_tokens_doc.apply(lambda x: ast.literal_eval(x))\n",
    "df.word_tokens = df.word_tokens.apply(lambda x: ast.literal_eval(x))\n",
    "df.norm_tokens = df.norm_tokens.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>word_tokens_doc</th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[I stumbled across this great restaurant overl...</td>\n",
       "      <td>[(I, stumbled, across, this, great, restaurant...</td>\n",
       "      <td>[(stumbl, across, great, restaur, overlook, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Excellent view on the ocean at sunset., Excel...</td>\n",
       "      <td>[(Excellent, view, on, the, ocean, at, sunset,...</td>\n",
       "      <td>[(excel, view, ocean, sunset), (excel, food), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[This place was not what the reviews portrayed...</td>\n",
       "      <td>[(This, place, was, not, what, the, reviews, p...</td>\n",
       "      <td>[(place, review, portray), (starter, walk, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[We were excited to repeat our Keoki's (in Kau...</td>\n",
       "      <td>[(We, were, excited, to, repeat, our, Keoki, '...</td>\n",
       "      <td>[(excit, repeat, keoki, 's, kauai, lovefest, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[If you're looking for a tourist spot, this is...</td>\n",
       "      <td>[(If, you, 're, looking, for, a, tourist, spot...</td>\n",
       "      <td>[('re, look, tourist, spot), (unfortun, could,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sent_tokens  \\\n",
       "0  [I stumbled across this great restaurant overl...   \n",
       "1  [Excellent view on the ocean at sunset., Excel...   \n",
       "2  [This place was not what the reviews portrayed...   \n",
       "3  [We were excited to repeat our Keoki's (in Kau...   \n",
       "4  [If you're looking for a tourist spot, this is...   \n",
       "\n",
       "                                     word_tokens_doc  \\\n",
       "0  [(I, stumbled, across, this, great, restaurant...   \n",
       "1  [(Excellent, view, on, the, ocean, at, sunset,...   \n",
       "2  [(This, place, was, not, what, the, reviews, p...   \n",
       "3  [(We, were, excited, to, repeat, our, Keoki, '...   \n",
       "4  [(If, you, 're, looking, for, a, tourist, spot...   \n",
       "\n",
       "                                     norm_tokens_doc  \n",
       "0  [(stumbl, across, great, restaur, overlook, oc...  \n",
       "1  [(excel, view, ocean, sunset), (excel, food), ...  \n",
       "2  [(place, review, portray), (starter, walk, sta...  \n",
       "3  [(excit, repeat, keoki, 's, kauai, lovefest, s...  \n",
       "4  [('re, look, tourist, spot), (unfortun, could,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['sent_tokens', 'word_tokens_doc', 'norm_tokens_doc']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>dataPublished</th>\n",
       "      <th>description</th>\n",
       "      <th>author</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>chr_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>...</th>\n",
       "      <th>uppercase_cnt</th>\n",
       "      <th>punctuation_cnt</th>\n",
       "      <th>vocab_cnt</th>\n",
       "      <th>ratio_lexical</th>\n",
       "      <th>ratio_content</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>word_tokens_doc</th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>norm_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>I stumbled across this great restaurant overlo...</td>\n",
       "      <td>Bella L.</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>11</td>\n",
       "      <td>664</td>\n",
       "      <td>4.022222</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>76</td>\n",
       "      <td>0.637037</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>[I stumbled across this great restaurant overl...</td>\n",
       "      <td>[(I, stumbled, across, this, great, restaurant...</td>\n",
       "      <td>[(stumbl, across, great, restaur, overlook, oc...</td>\n",
       "      <td>[I, stumbled, across, this, great, restaurant,...</td>\n",
       "      <td>[stumbl, across, great, restaur, overlook, oce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>Excellent view on the ocean at sunset.\\nExcell...</td>\n",
       "      <td>Rachou A.</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>160</td>\n",
       "      <td>3.611111</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>[Excellent view on the ocean at sunset., Excel...</td>\n",
       "      <td>[(Excellent, view, on, the, ocean, at, sunset,...</td>\n",
       "      <td>[(excel, view, ocean, sunset), (excel, food), ...</td>\n",
       "      <td>[Excellent, view, on, the, ocean, at, sunset, ...</td>\n",
       "      <td>[excel, view, ocean, sunset, excel, food, fres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>This place was not what the reviews portrayed ...</td>\n",
       "      <td>Ozzetta B.</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>14</td>\n",
       "      <td>1229</td>\n",
       "      <td>3.596364</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>130</td>\n",
       "      <td>0.534545</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>[This place was not what the reviews portrayed...</td>\n",
       "      <td>[(This, place, was, not, what, the, reviews, p...</td>\n",
       "      <td>[(place, review, portray), (starter, walk, sta...</td>\n",
       "      <td>[This, place, was, not, what, the, reviews, po...</td>\n",
       "      <td>[place, review, portray, starter, walk, stair,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-08</td>\n",
       "      <td>We were excited to repeat our Keoki's (in Kaua...</td>\n",
       "      <td>Arleen C.</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "      <td>34</td>\n",
       "      <td>2226</td>\n",
       "      <td>3.783158</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>206</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.604211</td>\n",
       "      <td>[We were excited to repeat our Keoki's (in Kau...</td>\n",
       "      <td>[(We, were, excited, to, repeat, our, Keoki, '...</td>\n",
       "      <td>[(excit, repeat, keoki, 's, kauai, lovefest, s...</td>\n",
       "      <td>[We, were, excited, to, repeat, our, Keoki, 's...</td>\n",
       "      <td>[excit, repeat, keoki, 's, kauai, lovefest, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>If you're looking for a tourist spot, this is ...</td>\n",
       "      <td>Carol B.</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>776</td>\n",
       "      <td>3.732143</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>97</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.636905</td>\n",
       "      <td>[If you're looking for a tourist spot, this is...</td>\n",
       "      <td>[(If, you, 're, looking, for, a, tourist, spot...</td>\n",
       "      <td>[('re, look, tourist, spot), (unfortun, could,...</td>\n",
       "      <td>[If, you, 're, looking, for, a, tourist, spot,...</td>\n",
       "      <td>['re, look, tourist, spot, unfortun, could, n'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                alias  ratingValue dataPublished  \\\n",
       "0  kimos-maui-lahaina            5    2019-01-06   \n",
       "1  kimos-maui-lahaina            5    2019-01-04   \n",
       "2  kimos-maui-lahaina            3    2018-12-25   \n",
       "3  kimos-maui-lahaina            2    2018-12-08   \n",
       "4  kimos-maui-lahaina            3    2018-11-29   \n",
       "\n",
       "                                         description      author  sentiment  \\\n",
       "0  I stumbled across this great restaurant overlo...    Bella L.          1   \n",
       "1  Excellent view on the ocean at sunset.\\nExcell...   Rachou A.          1   \n",
       "2  This place was not what the reviews portrayed ...  Ozzetta B.          0   \n",
       "3  We were excited to repeat our Keoki's (in Kaua...   Arleen C.          0   \n",
       "4  If you're looking for a tourist spot, this is ...    Carol B.          0   \n",
       "\n",
       "   word_count  sent_count  chr_count  avg_word_len  \\\n",
       "0         135          11        664      4.022222   \n",
       "1          36           5        160      3.611111   \n",
       "2         275          14       1229      3.596364   \n",
       "3         475          34       2226      3.783158   \n",
       "4         168          10        776      3.732143   \n",
       "\n",
       "                         ...                          uppercase_cnt  \\\n",
       "0                        ...                                     10   \n",
       "1                        ...                                      0   \n",
       "2                        ...                                      5   \n",
       "3                        ...                                      7   \n",
       "4                        ...                                      1   \n",
       "\n",
       "   punctuation_cnt  vocab_cnt  ratio_lexical  ratio_content  \\\n",
       "0               17         76       0.637037       0.674074   \n",
       "1                6         25       0.777778       0.638889   \n",
       "2               31        130       0.534545       0.567273   \n",
       "3               43        206       0.484211       0.604211   \n",
       "4               21         97       0.648810       0.636905   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [I stumbled across this great restaurant overl...   \n",
       "1  [Excellent view on the ocean at sunset., Excel...   \n",
       "2  [This place was not what the reviews portrayed...   \n",
       "3  [We were excited to repeat our Keoki's (in Kau...   \n",
       "4  [If you're looking for a tourist spot, this is...   \n",
       "\n",
       "                                     word_tokens_doc  \\\n",
       "0  [(I, stumbled, across, this, great, restaurant...   \n",
       "1  [(Excellent, view, on, the, ocean, at, sunset,...   \n",
       "2  [(This, place, was, not, what, the, reviews, p...   \n",
       "3  [(We, were, excited, to, repeat, our, Keoki, '...   \n",
       "4  [(If, you, 're, looking, for, a, tourist, spot...   \n",
       "\n",
       "                                     norm_tokens_doc  \\\n",
       "0  [(stumbl, across, great, restaur, overlook, oc...   \n",
       "1  [(excel, view, ocean, sunset), (excel, food), ...   \n",
       "2  [(place, review, portray), (starter, walk, sta...   \n",
       "3  [(excit, repeat, keoki, 's, kauai, lovefest, s...   \n",
       "4  [('re, look, tourist, spot), (unfortun, could,...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [I, stumbled, across, this, great, restaurant,...   \n",
       "1  [Excellent, view, on, the, ocean, at, sunset, ...   \n",
       "2  [This, place, was, not, what, the, reviews, po...   \n",
       "3  [We, were, excited, to, repeat, our, Keoki, 's...   \n",
       "4  [If, you, 're, looking, for, a, tourist, spot,...   \n",
       "\n",
       "                                         norm_tokens  \n",
       "0  [stumbl, across, great, restaur, overlook, oce...  \n",
       "1  [excel, view, ocean, sunset, excel, food, fres...  \n",
       "2  [place, review, portray, starter, walk, stair,...  \n",
       "3  [excit, repeat, keoki, 's, kauai, lovefest, si...  \n",
       "4  ['re, look, tourist, spot, unfortun, could, n'...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document is Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [stumbl, across, great, restaur, overlook, oce...\n",
       "1    [excel, view, ocean, sunset, excel, food, fres...\n",
       "2    [place, review, portray, starter, walk, stair,...\n",
       "3    [excit, repeat, keoki, 's, kauai, lovefest, si...\n",
       "4    ['re, look, tourist, spot, unfortun, could, n'...\n",
       "Name: norm_tokens, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documents are reviews\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "data = df.norm_tokens[:100]\n",
    "\n",
    "id2word = Dictionary(documents=data)\n",
    "doc_term_matrix = [id2word.doc2bow(doc) for doc in data]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "lda_model = Lda(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=id2word,\n",
    "    num_topics=10, \n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.101426671191072 , 0.27825371332849375\n"
     ]
    }
   ],
   "source": [
    "# lda test\n",
    "perplexity = lda_model.log_perplexity(doc_term_matrix)\n",
    "coherence = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v').get_coherence()\n",
    "print(perplexity, ',', coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32474995085991565"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimal topic numbers\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "for num_topics in range(2, 40, 4):\n",
    "    model = Lda(\n",
    "        corpus=doc_term_matrix,\n",
    "        id2word=id2word,\n",
    "        num_topics=num_topics, \n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True\n",
    "    )\n",
    "    model_list.append(model)\n",
    "    coherencemodel = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "optimal_model, optimal_coherence = max(((i,y) for i,y in zip(model_list, coherence_values)), key=lambda x: x[1])\n",
    "optimal_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x1a36a0ecf0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((i,y) for i,y in zip(model_list, coherence_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to select number of topics\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(2, 40, 4)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '../../model/mallet-2.0.8/bin/mallet.bat'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []; b = []\n",
    "for doc_topic_list in lda_model.get_document_topics(doc_term_matrix):\n",
    "    dtp = max(doc_topic_list, key=lambda x: x[1])\n",
    "    a.append(dtp)\n",
    "    doc_topic_list.remove(dtp)\n",
    "    b.append(doc_topic_list)\n",
    "pd.concat([df.iloc[:100,:], pd.DataFrame(a)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document is Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "data = pd.Series(list(chain(*df.norm_tokens_doc)), name='norm_tokens_doc')\n",
    "\n",
    "id2word = Dictionary(documents=data)\n",
    "doc_term_matrix = [id2word.doc2bow(doc) for doc in data]\n",
    "Lda = gensim.models.ldamulticore.LdaModel\n",
    "lda_model = Lda(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=id2word,\n",
    "    num_topics=10, \n",
    "    random_state=100,\n",
    "    update_every=1, # online iterative learning\n",
    "    chunksize=100,\n",
    "    passes=5,\n",
    "    distributed=False,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_word</th>\n",
       "      <th>0_prob</th>\n",
       "      <th>1_word</th>\n",
       "      <th>1_prob</th>\n",
       "      <th>2_word</th>\n",
       "      <th>2_prob</th>\n",
       "      <th>3_word</th>\n",
       "      <th>3_prob</th>\n",
       "      <th>4_word</th>\n",
       "      <th>4_prob</th>\n",
       "      <th>5_word</th>\n",
       "      <th>5_prob</th>\n",
       "      <th>6_word</th>\n",
       "      <th>6_prob</th>\n",
       "      <th>7_word</th>\n",
       "      <th>7_prob</th>\n",
       "      <th>8_word</th>\n",
       "      <th>8_prob</th>\n",
       "      <th>9_word</th>\n",
       "      <th>9_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vegan</td>\n",
       "      <td>0.077060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069492</td>\n",
       "      <td>'s</td>\n",
       "      <td>0.093777</td>\n",
       "      <td>sandwich</td>\n",
       "      <td>0.085427</td>\n",
       "      <td>realli</td>\n",
       "      <td>0.084974</td>\n",
       "      <td>food</td>\n",
       "      <td>0.079403</td>\n",
       "      <td>eat</td>\n",
       "      <td>0.054112</td>\n",
       "      <td>good</td>\n",
       "      <td>0.117601</td>\n",
       "      <td>one</td>\n",
       "      <td>0.064077</td>\n",
       "      <td>get</td>\n",
       "      <td>0.067296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>servic</td>\n",
       "      <td>0.070897</td>\n",
       "      <td>u</td>\n",
       "      <td>0.039674</td>\n",
       "      <td>time</td>\n",
       "      <td>0.064041</td>\n",
       "      <td>back</td>\n",
       "      <td>0.063367</td>\n",
       "      <td>got</td>\n",
       "      <td>0.068111</td>\n",
       "      <td>place</td>\n",
       "      <td>0.078227</td>\n",
       "      <td>friend</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>order</td>\n",
       "      <td>0.094104</td>\n",
       "      <td>meat</td>\n",
       "      <td>0.045037</td>\n",
       "      <td>'ve</td>\n",
       "      <td>0.040895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love</td>\n",
       "      <td>0.061557</td>\n",
       "      <td>nice</td>\n",
       "      <td>0.031822</td>\n",
       "      <td>go</td>\n",
       "      <td>0.057920</td>\n",
       "      <td>also</td>\n",
       "      <td>0.040185</td>\n",
       "      <td>amaz</td>\n",
       "      <td>0.066287</td>\n",
       "      <td>n't</td>\n",
       "      <td>0.075470</td>\n",
       "      <td>well</td>\n",
       "      <td>0.031912</td>\n",
       "      <td>chicken</td>\n",
       "      <td>0.045327</td>\n",
       "      <td>friendli</td>\n",
       "      <td>0.034419</td>\n",
       "      <td>best</td>\n",
       "      <td>0.040160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would</td>\n",
       "      <td>0.055005</td>\n",
       "      <td>ask</td>\n",
       "      <td>0.028222</td>\n",
       "      <td>definit</td>\n",
       "      <td>0.036695</td>\n",
       "      <td>taco</td>\n",
       "      <td>0.038857</td>\n",
       "      <td>sauc</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>great</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>pretti</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>menu</td>\n",
       "      <td>0.041703</td>\n",
       "      <td>think</td>\n",
       "      <td>0.024824</td>\n",
       "      <td>wait</td>\n",
       "      <td>0.039190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>''</td>\n",
       "      <td>0.048022</td>\n",
       "      <td>super</td>\n",
       "      <td>0.025552</td>\n",
       "      <td>'m</td>\n",
       "      <td>0.034372</td>\n",
       "      <td>fri</td>\n",
       "      <td>0.038612</td>\n",
       "      <td>came</td>\n",
       "      <td>0.050629</td>\n",
       "      <td>like</td>\n",
       "      <td>0.043326</td>\n",
       "      <td>much</td>\n",
       "      <td>0.028032</td>\n",
       "      <td>happi</td>\n",
       "      <td>0.028543</td>\n",
       "      <td>favorit</td>\n",
       "      <td>0.024703</td>\n",
       "      <td>make</td>\n",
       "      <td>0.037885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>``</td>\n",
       "      <td>0.047359</td>\n",
       "      <td>tabl</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>recommend</td>\n",
       "      <td>0.026549</td>\n",
       "      <td>fish</td>\n",
       "      <td>0.033766</td>\n",
       "      <td>drink</td>\n",
       "      <td>0.041031</td>\n",
       "      <td>tri</td>\n",
       "      <td>0.040802</td>\n",
       "      <td>star</td>\n",
       "      <td>0.024999</td>\n",
       "      <td>burger</td>\n",
       "      <td>0.024290</td>\n",
       "      <td>say</td>\n",
       "      <td>0.024316</td>\n",
       "      <td>even</td>\n",
       "      <td>0.037746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>price</td>\n",
       "      <td>0.029946</td>\n",
       "      <td>server</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>first</td>\n",
       "      <td>0.026456</td>\n",
       "      <td>shrimp</td>\n",
       "      <td>0.020907</td>\n",
       "      <td>enjoy</td>\n",
       "      <td>0.027594</td>\n",
       "      <td>come</td>\n",
       "      <td>0.033598</td>\n",
       "      <td>lot</td>\n",
       "      <td>0.021448</td>\n",
       "      <td>went</td>\n",
       "      <td>0.021026</td>\n",
       "      <td>take</td>\n",
       "      <td>0.024107</td>\n",
       "      <td>bread</td>\n",
       "      <td>0.030267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>could</td>\n",
       "      <td>0.029137</td>\n",
       "      <td>spot</td>\n",
       "      <td>0.021002</td>\n",
       "      <td>littl</td>\n",
       "      <td>0.023510</td>\n",
       "      <td>fresh</td>\n",
       "      <td>0.018318</td>\n",
       "      <td>next</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>delici</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>cream</td>\n",
       "      <td>0.020201</td>\n",
       "      <td>ca</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>better</td>\n",
       "      <td>0.023561</td>\n",
       "      <td>alway</td>\n",
       "      <td>0.028544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dish</td>\n",
       "      <td>0.026511</td>\n",
       "      <td>minut</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>thing</td>\n",
       "      <td>0.020422</td>\n",
       "      <td>still</td>\n",
       "      <td>0.017949</td>\n",
       "      <td>enough</td>\n",
       "      <td>0.019312</td>\n",
       "      <td>tast</td>\n",
       "      <td>0.022212</td>\n",
       "      <td>ice</td>\n",
       "      <td>0.019699</td>\n",
       "      <td>portion</td>\n",
       "      <td>0.019073</td>\n",
       "      <td>meal</td>\n",
       "      <td>0.021575</td>\n",
       "      <td>hour</td>\n",
       "      <td>0.028147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>everyth</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>right</td>\n",
       "      <td>0.018590</td>\n",
       "      <td>day</td>\n",
       "      <td>0.017563</td>\n",
       "      <td>dutch</td>\n",
       "      <td>0.015392</td>\n",
       "      <td>bbq</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>restaur</td>\n",
       "      <td>0.021401</td>\n",
       "      <td>way</td>\n",
       "      <td>0.019353</td>\n",
       "      <td>salad</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>work</td>\n",
       "      <td>0.020077</td>\n",
       "      <td>staff</td>\n",
       "      <td>0.025489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>peopl</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>locat</td>\n",
       "      <td>0.016615</td>\n",
       "      <td>never</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>wrap</td>\n",
       "      <td>0.014980</td>\n",
       "      <td>sea</td>\n",
       "      <td>0.015195</td>\n",
       "      <td>flavor</td>\n",
       "      <td>0.020729</td>\n",
       "      <td>need</td>\n",
       "      <td>0.017412</td>\n",
       "      <td>crunch</td>\n",
       "      <td>0.016974</td>\n",
       "      <td>said</td>\n",
       "      <td>0.019299</td>\n",
       "      <td>custom</td>\n",
       "      <td>0.023563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sweet</td>\n",
       "      <td>0.022134</td>\n",
       "      <td>help</td>\n",
       "      <td>0.016089</td>\n",
       "      <td>review</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>everi</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>let</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>want</td>\n",
       "      <td>0.017488</td>\n",
       "      <td>worth</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>small</td>\n",
       "      <td>0.016886</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018381</td>\n",
       "      <td>know</td>\n",
       "      <td>0.022783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>give</td>\n",
       "      <td>0.021956</td>\n",
       "      <td>guy</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>tasti</td>\n",
       "      <td>0.013129</td>\n",
       "      <td>chees</td>\n",
       "      <td>0.014226</td>\n",
       "      <td>expect</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>look</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>park</td>\n",
       "      <td>0.014254</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016329</td>\n",
       "      <td>call</td>\n",
       "      <td>0.015811</td>\n",
       "      <td>ever</td>\n",
       "      <td>0.022555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rice</td>\n",
       "      <td>0.019358</td>\n",
       "      <td>seem</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>big</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>bacon</td>\n",
       "      <td>0.013638</td>\n",
       "      <td>water</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>'re</td>\n",
       "      <td>0.012646</td>\n",
       "      <td>plate</td>\n",
       "      <td>0.013944</td>\n",
       "      <td>option</td>\n",
       "      <td>0.013960</td>\n",
       "      <td>someth</td>\n",
       "      <td>0.015375</td>\n",
       "      <td>sure</td>\n",
       "      <td>0.019110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>experi</td>\n",
       "      <td>0.016614</td>\n",
       "      <td>anoth</td>\n",
       "      <td>0.014020</td>\n",
       "      <td>long</td>\n",
       "      <td>0.012293</td>\n",
       "      <td>differ</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>bring</td>\n",
       "      <td>0.012654</td>\n",
       "      <td>made</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>around</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>special</td>\n",
       "      <td>0.013272</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015019</td>\n",
       "      <td>awesom</td>\n",
       "      <td>0.017661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>side</td>\n",
       "      <td>0.016078</td>\n",
       "      <td>busi</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>see</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>garlic</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>high</td>\n",
       "      <td>0.011870</td>\n",
       "      <td>lunch</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>real</td>\n",
       "      <td>0.013553</td>\n",
       "      <td>beef</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>check</td>\n",
       "      <td>0.014972</td>\n",
       "      <td>seat</td>\n",
       "      <td>0.014114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>qualiti</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>walk</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>item</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>deep</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>boyfriend</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>two</td>\n",
       "      <td>0.010514</td>\n",
       "      <td>huge</td>\n",
       "      <td>0.012251</td>\n",
       "      <td>larg</td>\n",
       "      <td>0.011218</td>\n",
       "      <td>open</td>\n",
       "      <td>0.014972</td>\n",
       "      <td>took</td>\n",
       "      <td>0.014063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>potato</td>\n",
       "      <td>0.013278</td>\n",
       "      <td>perfect</td>\n",
       "      <td>0.013417</td>\n",
       "      <td>cook</td>\n",
       "      <td>0.011946</td>\n",
       "      <td>spici</td>\n",
       "      <td>0.009801</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>'ll</td>\n",
       "      <td>0.010126</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012160</td>\n",
       "      <td>half</td>\n",
       "      <td>0.011134</td>\n",
       "      <td>top</td>\n",
       "      <td>0.014428</td>\n",
       "      <td>serv</td>\n",
       "      <td>0.013527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>veggi</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>area</td>\n",
       "      <td>0.012729</td>\n",
       "      <td>mani</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>full</td>\n",
       "      <td>0.009617</td>\n",
       "      <td>outsid</td>\n",
       "      <td>0.010389</td>\n",
       "      <td>sinc</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>stop</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>size</td>\n",
       "      <td>0.010833</td>\n",
       "      <td>bad</td>\n",
       "      <td>0.013451</td>\n",
       "      <td>though</td>\n",
       "      <td>0.013377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>usual</td>\n",
       "      <td>0.010899</td>\n",
       "      <td>told</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>cool</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>god</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>late</td>\n",
       "      <td>0.010230</td>\n",
       "      <td>new</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>close</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>crispi</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.012203</td>\n",
       "      <td>excel</td>\n",
       "      <td>0.012287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0_word    0_prob   1_word    1_prob     2_word    2_prob    3_word  \\\n",
       "0     vegan  0.077060      ...  0.069492         's  0.093777  sandwich   \n",
       "1    servic  0.070897        u  0.039674       time  0.064041      back   \n",
       "2      love  0.061557     nice  0.031822         go  0.057920      also   \n",
       "3     would  0.055005      ask  0.028222    definit  0.036695      taco   \n",
       "4        ''  0.048022    super  0.025552         'm  0.034372       fri   \n",
       "5        ``  0.047359     tabl  0.023726  recommend  0.026549      fish   \n",
       "6     price  0.029946   server  0.021025      first  0.026456    shrimp   \n",
       "7     could  0.029137     spot  0.021002      littl  0.023510     fresh   \n",
       "8      dish  0.026511    minut  0.020985      thing  0.020422     still   \n",
       "9   everyth  0.023457    right  0.018590        day  0.017563     dutch   \n",
       "10    peopl  0.022327    locat  0.016615      never  0.017300      wrap   \n",
       "11    sweet  0.022134     help  0.016089     review  0.014233     everi   \n",
       "12     give  0.021956      guy  0.015439      tasti  0.013129     chees   \n",
       "13     rice  0.019358     seem  0.014356        big  0.012577     bacon   \n",
       "14   experi  0.016614    anoth  0.014020       long  0.012293    differ   \n",
       "15     side  0.016078     busi  0.013971        see  0.012163    garlic   \n",
       "16  qualiti  0.013682     walk  0.013656       item  0.011971      deep   \n",
       "17   potato  0.013278  perfect  0.013417       cook  0.011946     spici   \n",
       "18    veggi  0.011552     area  0.012729       mani  0.011481      full   \n",
       "19    usual  0.010899     told  0.012231       cool  0.011333       god   \n",
       "\n",
       "      3_prob     4_word    4_prob   5_word    5_prob  6_word    6_prob  \\\n",
       "0   0.085427     realli  0.084974     food  0.079403     eat  0.054112   \n",
       "1   0.063367        got  0.068111    place  0.078227  friend  0.032118   \n",
       "2   0.040185       amaz  0.066287      n't  0.075470    well  0.031912   \n",
       "3   0.038857       sauc  0.056497    great  0.046948  pretti  0.028812   \n",
       "4   0.038612       came  0.050629     like  0.043326    much  0.028032   \n",
       "5   0.033766      drink  0.041031      tri  0.040802    star  0.024999   \n",
       "6   0.020907      enjoy  0.027594     come  0.033598     lot  0.021448   \n",
       "7   0.018318       next  0.023289   delici  0.026355   cream  0.020201   \n",
       "8   0.017949     enough  0.019312     tast  0.022212     ice  0.019699   \n",
       "9   0.015392        bbq  0.018598  restaur  0.021401     way  0.019353   \n",
       "10  0.014980        sea  0.015195   flavor  0.020729    need  0.017412   \n",
       "11  0.014378        let  0.014377     want  0.017488   worth  0.017007   \n",
       "12  0.014226     expect  0.013575     look  0.014875    park  0.014254   \n",
       "13  0.013638      water  0.012682      're  0.012646   plate  0.013944   \n",
       "14  0.012650      bring  0.012654     made  0.012512  around  0.013777   \n",
       "15  0.011628       high  0.011870    lunch  0.010867    real  0.013553   \n",
       "16  0.010036  boyfriend  0.011436      two  0.010514    huge  0.012251   \n",
       "17  0.009801         ok  0.011044      'll  0.010126       4  0.012160   \n",
       "18  0.009617     outsid  0.010389     sinc  0.009927    stop  0.011903   \n",
       "19  0.009217       late  0.010230      new  0.009425   close  0.011750   \n",
       "\n",
       "     7_word    7_prob    8_word    8_prob  9_word    9_prob  \n",
       "0      good  0.117601       one  0.064077     get  0.067296  \n",
       "1     order  0.094104      meat  0.045037     've  0.040895  \n",
       "2   chicken  0.045327  friendli  0.034419    best  0.040160  \n",
       "3      menu  0.041703     think  0.024824    wait  0.039190  \n",
       "4     happi  0.028543   favorit  0.024703    make  0.037885  \n",
       "5    burger  0.024290       say  0.024316    even  0.037746  \n",
       "6      went  0.021026      take  0.024107   bread  0.030267  \n",
       "7        ca  0.020716    better  0.023561   alway  0.028544  \n",
       "8   portion  0.019073      meal  0.021575    hour  0.028147  \n",
       "9     salad  0.017363      work  0.020077   staff  0.025489  \n",
       "10   crunch  0.016974      said  0.019299  custom  0.023563  \n",
       "11    small  0.016886         2  0.018381    know  0.022783  \n",
       "12        5  0.016329      call  0.015811    ever  0.022555  \n",
       "13   option  0.013960    someth  0.015375    sure  0.019110  \n",
       "14  special  0.013272         3  0.015019  awesom  0.017661  \n",
       "15     beef  0.012740     check  0.014972    seat  0.014114  \n",
       "16     larg  0.011218      open  0.014972    took  0.014063  \n",
       "17     half  0.011134       top  0.014428    serv  0.013527  \n",
       "18     size  0.010833       bad  0.013451  though  0.013377  \n",
       "19   crispi  0.009439     pizza  0.012203   excel  0.012287  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words = lda_model.show_topics(formatted=False, num_words=20)\n",
    "\n",
    "topics_list = []\n",
    "for (topic, word_list) in topic_words:\n",
    "    temp = pd.DataFrame(word_list, columns=[f'{topic}_word', f'{topic}_prob'])\n",
    "    topics_list.append(temp)\n",
    "pd.concat(topics_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Word</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>vegan</td>\n",
       "      <td>0.077060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>servic</td>\n",
       "      <td>0.070897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>love</td>\n",
       "      <td>0.061557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>would</td>\n",
       "      <td>0.055005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>''</td>\n",
       "      <td>0.048022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>``</td>\n",
       "      <td>0.047359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>price</td>\n",
       "      <td>0.029946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>could</td>\n",
       "      <td>0.029137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>dish</td>\n",
       "      <td>0.026511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>everyth</td>\n",
       "      <td>0.023457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>u</td>\n",
       "      <td>0.039674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>nice</td>\n",
       "      <td>0.031822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>ask</td>\n",
       "      <td>0.028222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>super</td>\n",
       "      <td>0.025552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>tabl</td>\n",
       "      <td>0.023726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>server</td>\n",
       "      <td>0.021025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>spot</td>\n",
       "      <td>0.021002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>minut</td>\n",
       "      <td>0.020985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>right</td>\n",
       "      <td>0.018590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>'s</td>\n",
       "      <td>0.093777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>time</td>\n",
       "      <td>0.064041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>go</td>\n",
       "      <td>0.057920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>definit</td>\n",
       "      <td>0.036695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>'m</td>\n",
       "      <td>0.034372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>recommend</td>\n",
       "      <td>0.026549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>first</td>\n",
       "      <td>0.026456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>littl</td>\n",
       "      <td>0.023510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>thing</td>\n",
       "      <td>0.020421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>day</td>\n",
       "      <td>0.017563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>7</td>\n",
       "      <td>good</td>\n",
       "      <td>0.117601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>7</td>\n",
       "      <td>order</td>\n",
       "      <td>0.094104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7</td>\n",
       "      <td>chicken</td>\n",
       "      <td>0.045327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7</td>\n",
       "      <td>menu</td>\n",
       "      <td>0.041703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>7</td>\n",
       "      <td>happi</td>\n",
       "      <td>0.028543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>7</td>\n",
       "      <td>burger</td>\n",
       "      <td>0.024290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7</td>\n",
       "      <td>went</td>\n",
       "      <td>0.021026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>7</td>\n",
       "      <td>ca</td>\n",
       "      <td>0.020716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>7</td>\n",
       "      <td>portion</td>\n",
       "      <td>0.019073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>7</td>\n",
       "      <td>salad</td>\n",
       "      <td>0.017363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>8</td>\n",
       "      <td>one</td>\n",
       "      <td>0.064077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>8</td>\n",
       "      <td>meat</td>\n",
       "      <td>0.045037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>8</td>\n",
       "      <td>friendli</td>\n",
       "      <td>0.034419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>8</td>\n",
       "      <td>think</td>\n",
       "      <td>0.024824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>8</td>\n",
       "      <td>favorit</td>\n",
       "      <td>0.024703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>8</td>\n",
       "      <td>say</td>\n",
       "      <td>0.024316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>8</td>\n",
       "      <td>take</td>\n",
       "      <td>0.024107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>8</td>\n",
       "      <td>better</td>\n",
       "      <td>0.023561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>8</td>\n",
       "      <td>meal</td>\n",
       "      <td>0.021575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>8</td>\n",
       "      <td>work</td>\n",
       "      <td>0.020077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>9</td>\n",
       "      <td>get</td>\n",
       "      <td>0.067296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>9</td>\n",
       "      <td>'ve</td>\n",
       "      <td>0.040895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>9</td>\n",
       "      <td>best</td>\n",
       "      <td>0.040160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>9</td>\n",
       "      <td>wait</td>\n",
       "      <td>0.039190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>9</td>\n",
       "      <td>make</td>\n",
       "      <td>0.037885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9</td>\n",
       "      <td>even</td>\n",
       "      <td>0.037746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>9</td>\n",
       "      <td>bread</td>\n",
       "      <td>0.030267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>9</td>\n",
       "      <td>alway</td>\n",
       "      <td>0.028544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9</td>\n",
       "      <td>hour</td>\n",
       "      <td>0.028147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>9</td>\n",
       "      <td>staff</td>\n",
       "      <td>0.025489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic       Word         P\n",
       "0       0      vegan  0.077060\n",
       "1       0     servic  0.070897\n",
       "2       0       love  0.061557\n",
       "3       0      would  0.055005\n",
       "4       0         ''  0.048022\n",
       "5       0         ``  0.047359\n",
       "6       0      price  0.029946\n",
       "7       0      could  0.029137\n",
       "8       0       dish  0.026511\n",
       "9       0    everyth  0.023457\n",
       "10      1        ...  0.069492\n",
       "11      1          u  0.039674\n",
       "12      1       nice  0.031822\n",
       "13      1        ask  0.028222\n",
       "14      1      super  0.025552\n",
       "15      1       tabl  0.023726\n",
       "16      1     server  0.021025\n",
       "17      1       spot  0.021002\n",
       "18      1      minut  0.020985\n",
       "19      1      right  0.018590\n",
       "20      2         's  0.093777\n",
       "21      2       time  0.064041\n",
       "22      2         go  0.057920\n",
       "23      2    definit  0.036695\n",
       "24      2         'm  0.034372\n",
       "25      2  recommend  0.026549\n",
       "26      2      first  0.026456\n",
       "27      2      littl  0.023510\n",
       "28      2      thing  0.020421\n",
       "29      2        day  0.017563\n",
       "..    ...        ...       ...\n",
       "70      7       good  0.117601\n",
       "71      7      order  0.094104\n",
       "72      7    chicken  0.045327\n",
       "73      7       menu  0.041703\n",
       "74      7      happi  0.028543\n",
       "75      7     burger  0.024290\n",
       "76      7       went  0.021026\n",
       "77      7         ca  0.020716\n",
       "78      7    portion  0.019073\n",
       "79      7      salad  0.017363\n",
       "80      8        one  0.064077\n",
       "81      8       meat  0.045037\n",
       "82      8   friendli  0.034419\n",
       "83      8      think  0.024824\n",
       "84      8    favorit  0.024703\n",
       "85      8        say  0.024316\n",
       "86      8       take  0.024107\n",
       "87      8     better  0.023561\n",
       "88      8       meal  0.021575\n",
       "89      8       work  0.020077\n",
       "90      9        get  0.067296\n",
       "91      9        've  0.040895\n",
       "92      9       best  0.040160\n",
       "93      9       wait  0.039190\n",
       "94      9       make  0.037885\n",
       "95      9       even  0.037746\n",
       "96      9      bread  0.030267\n",
       "97      9      alway  0.028544\n",
       "98      9       hour  0.028147\n",
       "99      9      staff  0.025489\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_per_topic = []\n",
    "for t in range(lda_model.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 10)])\n",
    "\n",
    "pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.077*\"vegan\" + 0.071*\"servic\" + 0.062*\"love\" + 0.055*\"would\"'),\n",
       " (1, '0.069*\"...\" + 0.040*\"u\" + 0.032*\"nice\" + 0.028*\"ask\"'),\n",
       " (2, '0.094*\"\\'s\" + 0.064*\"time\" + 0.058*\"go\" + 0.037*\"definit\"'),\n",
       " (3, '0.085*\"sandwich\" + 0.063*\"back\" + 0.040*\"also\" + 0.039*\"taco\"'),\n",
       " (4, '0.085*\"realli\" + 0.068*\"got\" + 0.066*\"amaz\" + 0.056*\"sauc\"'),\n",
       " (5, '0.079*\"food\" + 0.078*\"place\" + 0.075*\"n\\'t\" + 0.047*\"great\"'),\n",
       " (6, '0.054*\"eat\" + 0.032*\"friend\" + 0.032*\"well\" + 0.029*\"pretti\"'),\n",
       " (7, '0.118*\"good\" + 0.094*\"order\" + 0.045*\"chicken\" + 0.042*\"menu\"'),\n",
       " (8, '0.064*\"one\" + 0.045*\"meat\" + 0.034*\"friendli\" + 0.025*\"think\"'),\n",
       " (9, '0.067*\"get\" + 0.041*\"\\'ve\" + 0.040*\"best\" + 0.039*\"wait\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = '../../model/lda_model_0001_0256_simple_doc2bow'\n",
    "# save\n",
    "lda_model.save(fname)\n",
    "\n",
    "# load\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "saved_model = Lda.load(fname)\n",
    "saved_model.show_topics(num_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454043"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.norm_tokens_doc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357040"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda_model.get_document_topics(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(stumbl, across, great, restaur, overlook, oce...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.311255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(high, expect, place, boy, blow, water)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.202032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(fish, chip, best, 've, ever, 've, lot, includ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.202616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(highli, recommend)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.166164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(also, turkey, bacon, sandwich, good)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.191926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     norm_tokens_doc  topic      prob\n",
       "0  (stumbl, across, great, restaur, overlook, oce...      5  0.311255\n",
       "1            (high, expect, place, boy, blow, water)      5  0.202032\n",
       "2  (fish, chip, best, 've, ever, 've, lot, includ...      9  0.202616\n",
       "3                                (highli, recommend)      5  0.166164\n",
       "4              (also, turkey, bacon, sandwich, good)      3  0.191926"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score topic and probabilities to each document (sentence in this case)\n",
    "a = []; b = []\n",
    "for doc_topic_list in lda_model.get_document_topics(doc_term_matrix):\n",
    "    dtp = max(doc_topic_list, key=lambda x: x[1])\n",
    "    a.append(dtp)\n",
    "    doc_topic_list.remove(dtp)\n",
    "    b.append(doc_topic_list)\n",
    "\n",
    "pd_sent_topic = pd.concat([data, pd.DataFrame(a, columns=['topic', 'prob'])], axis=1)\n",
    "pd_sent_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "norm_tokens_doc    3357040\n",
       "topic              3357040\n",
       "prob               3357040\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sent_topic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.06538894),\n",
       " (1, 0.06996099),\n",
       " (2, 0.11178656),\n",
       " (3, 0.07341155),\n",
       " (4, 0.049342375),\n",
       " (5, 0.1372504),\n",
       " (6, 0.0695846),\n",
       " (7, 0.0727972),\n",
       " (8, 0.073354565),\n",
       " (9, 0.27712286)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document/review topic prob distribution\n",
    "lda_model[doc_term_matrix][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.06538894),\n",
       " (1, 0.06996099),\n",
       " (2, 0.11178656),\n",
       " (3, 0.07341155),\n",
       " (4, 0.049342375),\n",
       " (5, 0.1372504),\n",
       " (6, 0.0695846),\n",
       " (7, 0.0727972),\n",
       " (8, 0.073354565),\n",
       " (9, 0.27712286)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document/review topic prob distribution\n",
    "lda_model.get_document_topics(doc_term_matrix[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000000026077032"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(w for i, w in lda_model[doc_term_matrix][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stumbl',\n",
       " 'across',\n",
       " 'great',\n",
       " 'restaur',\n",
       " 'overlook',\n",
       " 'ocean',\n",
       " 'lunch',\n",
       " 'vacat',\n",
       " 'maui')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]\n",
    "# df.norm_tokens_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stumbl',\n",
       "  'across',\n",
       "  'great',\n",
       "  'restaur',\n",
       "  'overlook',\n",
       "  'ocean',\n",
       "  'lunch',\n",
       "  'vacat',\n",
       "  'maui'),\n",
       " ('high', 'expect', 'place', 'boy', 'blow', 'water'),\n",
       " ('fish', 'chip', 'best', \"'ve\", 'ever', \"'ve\", 'lot', 'includ', 'london'),\n",
       " ('highli', 'recommend'),\n",
       " ('also', 'turkey', 'bacon', 'sandwich', 'good'),\n",
       " ('term', 'drink', 'highli', 'recommend', 'pacif', 'paradis', 'drink'),\n",
       " ('delici', 'tropic'),\n",
       " ('also', 'realli', 'enjoy', 'lahaina', 'lemonad'),\n",
       " ('servic', 'realli', 'great'),\n",
       " ('wish',\n",
       "  'rememb',\n",
       "  'waitress',\n",
       "  'name',\n",
       "  'truli',\n",
       "  'awesom',\n",
       "  'recommend',\n",
       "  'best',\n",
       "  'stuff'),\n",
       " ('blond', 'cute', 'sunglass')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in df.norm_tokens_doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357040"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd_sent_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357040"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(i) for i in df.norm_tokens_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 14, 34, 10, 6, 6, 14, 8, 10]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in df.norm_tokens_doc[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(stumbl, across, great, restaur, overlook, oce...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.311255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(high, expect, place, boy, blow, water)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.202032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(fish, chip, best, 've, ever, 've, lot, includ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.202616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(highli, recommend)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.166164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(also, turkey, bacon, sandwich, good)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.191926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(term, drink, highli, recommend, pacif, paradi...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.182715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(delici, tropic)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.196258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(also, realli, enjoy, lahaina, lemonad)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.154845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(servic, realli, great)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.190524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(wish, rememb, waitress, name, truli, awesom, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.277123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(blond, cute, sunglass)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.161319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      norm_tokens_doc  topic      prob\n",
       "0   (stumbl, across, great, restaur, overlook, oce...      5  0.311255\n",
       "1             (high, expect, place, boy, blow, water)      5  0.202032\n",
       "2   (fish, chip, best, 've, ever, 've, lot, includ...      9  0.202616\n",
       "3                                 (highli, recommend)      5  0.166164\n",
       "4               (also, turkey, bacon, sandwich, good)      3  0.191926\n",
       "5   (term, drink, highli, recommend, pacif, paradi...      4  0.182715\n",
       "6                                    (delici, tropic)      5  0.196258\n",
       "7             (also, realli, enjoy, lahaina, lemonad)      5  0.154845\n",
       "8                             (servic, realli, great)      5  0.190524\n",
       "9   (wish, rememb, waitress, name, truli, awesom, ...      9  0.277123\n",
       "10                            (blond, cute, sunglass)      5  0.161319"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sent_topic.iloc[0:11,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(excel, view, ocean, sunset)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.241888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(excel, food)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.196258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(fresh, fish, coconut, yuzu, husband)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.219528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(love,)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.171319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(waitress, super, nice)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.161309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          norm_tokens_doc  topic      prob\n",
       "11           (excel, view, ocean, sunset)      5  0.241888\n",
       "12                          (excel, food)      5  0.196258\n",
       "13  (fresh, fish, coconut, yuzu, husband)      3  0.219528\n",
       "14                                (love,)      5  0.171319\n",
       "15                (waitress, super, nice)      5  0.161309"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sent_topic.iloc[11:16,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(place, review, portray)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.190569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(starter, walk, stair, want, sit, balconi, cat...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.265855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(told, ask, sit, tabl, clean, coupl, left)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(wait, anoth, server, walk, right, u, sat, ano...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.305434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(end, wait, anoth, tabl, place, 40, min, tabl,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.213793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(food, okay, ..., veget, option, ravioli, dish...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.172228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(tofu, although, look, like, nice, grill, piec...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.255196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>('ve, tofu, plenti, time, actual, get, pick, f...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.255196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(true, dish)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.166164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(husband, fish, dish, rice, veggi, side, sweet...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.209332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(unfortun, hash, thing, like)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.185116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>('ve, come, maui, almost, 8, year, like, tri, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.387395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(first, time, tri, kimo, 's, unfortun, last)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.228265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(sorry..l, ob)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.175424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      norm_tokens_doc  topic      prob\n",
       "16                           (place, review, portray)      5  0.190569\n",
       "17  (starter, walk, stair, want, sit, balconi, cat...      5  0.265855\n",
       "18         (told, ask, sit, tabl, clean, coupl, left)      1  0.256729\n",
       "19  (wait, anoth, server, walk, right, u, sat, ano...      1  0.305434\n",
       "20  (end, wait, anoth, tabl, place, 40, min, tabl,...      1  0.213793\n",
       "21  (food, okay, ..., veget, option, ravioli, dish...      7  0.172228\n",
       "22  (tofu, although, look, like, nice, grill, piec...      5  0.255196\n",
       "23  ('ve, tofu, plenti, time, actual, get, pick, f...      5  0.255196\n",
       "24                                       (true, dish)      5  0.166164\n",
       "25  (husband, fish, dish, rice, veggi, side, sweet...      0  0.209332\n",
       "26                      (unfortun, hash, thing, like)      5  0.185116\n",
       "27  ('ve, come, maui, almost, 8, year, like, tri, ...      5  0.387395\n",
       "28       (first, time, tri, kimo, 's, unfortun, last)      2  0.228265\n",
       "29                                     (sorry..l, ob)      5  0.175424"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sent_topic.iloc[16:30,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sentences and their topic/probs for the **first review**\n",
    "pd_sent_topic[pd_sent_topic.norm_tokens_doc.apply(lambda x: tuple(x)).isin([i for i in df.norm_tokens_doc[0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd_doc_temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1dbd9f9a040c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd_doc_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd_doc_temp' is not defined"
     ]
    }
   ],
   "source": [
    "pd_doc_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,4,6,5,6,6,3,6,3,1,8'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(map(str, pd_doc_temp.topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# This is buggy, because the .isin(inner_list) select multiple occurances of inner_list and list populates with\n",
    "# out of document sentences..\n",
    "# 03/20/2019\n",
    "#\n",
    "topic_mode = []\n",
    "topic_list = []\n",
    "sent_topics = []\n",
    "# score topic and probabilities to each document (**review** in this case)\n",
    "for doc_sents in df.norm_tokens_doc[:100]:    \n",
    "    # pd_doc_temp: the topic and the prob for the current document/ review: all sentences and their topic and probs\n",
    "    pd_doc_temp = pd_sent_topic[pd_sent_topic.norm_tokens_doc.apply(lambda x: tuple(x)).isin([tuple(i) for i in doc_sents])]\n",
    "    #Â find the most frequent topic in the review\n",
    "    sent_topics.append(pd_doc_temp.apply(lambda x: (x.topic, x.prob), axis=1).tolist())\n",
    "    topic_mode.append(round(pd_doc_temp.topic.mode()[0]))    \n",
    "    topic_list.append(','.join(map(str, pd_doc_temp.topic)))\n",
    "    \n",
    "df['sent_topics'] = pd.Series(sent_topics, name='sent_topics')\n",
    "df['topic_mode'] = pd.Series(topic_mode, name='topic_mode')\n",
    "df['topic_list'] = pd.Series(topic_list, name='topic_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# This works since we are using indices of current document on the lda's documents\n",
    "# 03/21/2019\n",
    "#\n",
    "topic_mode = []\n",
    "topic_list = []\n",
    "sent_topics = []\n",
    "i, j = 0, 0\n",
    "# score topic and probabilities to each document (**review** in this case)\n",
    "for doc_sents in df.norm_tokens_doc:\n",
    "    j = j + len(doc_sents)\n",
    "    pd_doc_temp = pd_sent_topic.iloc[i:j,:]\n",
    "    i = j\n",
    "    \n",
    "    sent_topics.append(pd_doc_temp.apply(lambda x: (x.topic, x.prob), axis=1).tolist())\n",
    "    topic_mode.append(round(pd_doc_temp.topic.mode()[0]))    \n",
    "    topic_list.append(','.join(map(str, pd_doc_temp.topic)))\n",
    "    \n",
    "df['sent_topics'] = pd.Series(sent_topics, name='sent_topics')\n",
    "df['topic_mode'] = pd.Series(topic_mode, name='topic_mode')\n",
    "df['topic_list'] = pd.Series(topic_list, name='topic_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357035     (5, 0.1962578445672989)\n",
       "3357036    (5, 0.16130907833576202)\n",
       "3357037    (5, 0.18000635504722595)\n",
       "3357038    (5, 0.16624051332473755)\n",
       "3357039    (5, 0.21973922848701477)\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_doc_temp.apply(lambda x: (x.topic, x.prob), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136022</th>\n",
       "      <td>(husband, surpris, birthday, cake, kristi)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.147207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136023</th>\n",
       "      <td>(last, minut, abl, make, happen)</td>\n",
       "      <td>4</td>\n",
       "      <td>0.148071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136024</th>\n",
       "      <td>(unfortun, n't, like, pineappl, cake)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.142321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136025</th>\n",
       "      <td>(thought, kind, dri, side, also, may, fridg, d...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.153565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136026</th>\n",
       "      <td>(cake, gluten, free)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.185749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136027</th>\n",
       "      <td>('m, sure, realiz, specialti, bakeri)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.173672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136028</th>\n",
       "      <td>(like, carrot, cake, 's, fave)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.126636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136029</th>\n",
       "      <td>(anyway, sweet, gestur, husband, 's, part, eve...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.139029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136030</th>\n",
       "      <td>(thank, kristi, short, notic, cake)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.132093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136031</th>\n",
       "      <td>('m, sure, could, 've, gotten, someth, like, w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136032</th>\n",
       "      <td>(cream, chees, frost)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.170084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          norm_tokens_doc  topic      prob\n",
       "136022         (husband, surpris, birthday, cake, kristi)      6  0.147207\n",
       "136023                   (last, minut, abl, make, happen)      4  0.148071\n",
       "136024              (unfortun, n't, like, pineappl, cake)      6  0.142321\n",
       "136025  (thought, kind, dri, side, also, may, fridg, d...      5  0.153565\n",
       "136026                               (cake, gluten, free)      6  0.185749\n",
       "136027              ('m, sure, realiz, specialti, bakeri)      6  0.173672\n",
       "136028                     (like, carrot, cake, 's, fave)      3  0.126636\n",
       "136029  (anyway, sweet, gestur, husband, 's, part, eve...      6  0.139029\n",
       "136030                (thank, kristi, short, notic, cake)      3  0.132093\n",
       "136031  ('m, sure, could, 've, gotten, someth, like, w...      1  0.175870\n",
       "136032                              (cream, chees, frost)      8  0.170084"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_doc_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>word_tokens_doc</th>\n",
       "      <th>topic_mode</th>\n",
       "      <th>topic_list</th>\n",
       "      <th>sent_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>5</td>\n",
       "      <td>[(I, stumbled, across, this, great, restaurant...</td>\n",
       "      <td>5</td>\n",
       "      <td>5,5,9,5,3,4,5,5,5,9,5</td>\n",
       "      <td>[(5, 0.3112545907497406), (5, 0.20203207433223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>5</td>\n",
       "      <td>[(Excellent, view, on, the, ocean, at, sunset,...</td>\n",
       "      <td>5</td>\n",
       "      <td>5,5,3,5,5</td>\n",
       "      <td>[(5, 0.24188750982284546), (5, 0.1962578445672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>3</td>\n",
       "      <td>[(This, place, was, not, what, the, reviews, p...</td>\n",
       "      <td>5</td>\n",
       "      <td>5,5,1,1,1,7,5,5,5,0,5,5,2,5</td>\n",
       "      <td>[(5, 0.19056889414787292), (5, 0.2658553421497...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>2</td>\n",
       "      <td>[(We, were, excited, to, repeat, our, Keoki, '...</td>\n",
       "      <td>5</td>\n",
       "      <td>5,5,2,6,5,0,5,5,8,5,5,5,7,7,5,5,5,5,5,5,5,7,9,...</td>\n",
       "      <td>[(5, 0.18351130187511444), (5, 0.2023473381996...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>3</td>\n",
       "      <td>[(If, you, 're, looking, for, a, tourist, spot...</td>\n",
       "      <td>5</td>\n",
       "      <td>5,5,2,6,9,5,9,0,5,5</td>\n",
       "      <td>[(5, 0.21350185573101044), (5, 0.1800063550472...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                alias  ratingValue  \\\n",
       "0  kimos-maui-lahaina            5   \n",
       "1  kimos-maui-lahaina            5   \n",
       "2  kimos-maui-lahaina            3   \n",
       "3  kimos-maui-lahaina            2   \n",
       "4  kimos-maui-lahaina            3   \n",
       "\n",
       "                                     word_tokens_doc  topic_mode  \\\n",
       "0  [(I, stumbled, across, this, great, restaurant...           5   \n",
       "1  [(Excellent, view, on, the, ocean, at, sunset,...           5   \n",
       "2  [(This, place, was, not, what, the, reviews, p...           5   \n",
       "3  [(We, were, excited, to, repeat, our, Keoki, '...           5   \n",
       "4  [(If, you, 're, looking, for, a, tourist, spot...           5   \n",
       "\n",
       "                                          topic_list  \\\n",
       "0                              5,5,9,5,3,4,5,5,5,9,5   \n",
       "1                                          5,5,3,5,5   \n",
       "2                        5,5,1,1,1,7,5,5,5,0,5,5,2,5   \n",
       "3  5,5,2,6,5,0,5,5,8,5,5,5,7,7,5,5,5,5,5,5,5,7,9,...   \n",
       "4                                5,5,2,6,9,5,9,0,5,5   \n",
       "\n",
       "                                         sent_topics  \n",
       "0  [(5, 0.3112545907497406), (5, 0.20203207433223...  \n",
       "1  [(5, 0.24188750982284546), (5, 0.1962578445672...  \n",
       "2  [(5, 0.19056889414787292), (5, 0.2658553421497...  \n",
       "3  [(5, 0.18351130187511444), (5, 0.2023473381996...  \n",
       "4  [(5, 0.21350185573101044), (5, 0.1800063550472...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['alias', 'ratingValue', 'word_tokens_doc', 'topic_mode', 'topic_list', 'sent_topics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/processed/yp_competitors_rws_0001_0256_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_mode</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>442512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             alias\n",
       "topic_mode        \n",
       "0              804\n",
       "1             2379\n",
       "2             3186\n",
       "3             3570\n",
       "4               64\n",
       "5           442512\n",
       "6              446\n",
       "7              420\n",
       "8              180\n",
       "9              482"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['topic_mode', 'alias']].groupby(by='topic_mode').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[doc_term_matrix]):\n",
    "    topic_weights.append([w for i, w in row_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "arr.shape\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "# topic_num = np.argmax(arr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a 2d PCA model to the vectors\n",
    "pca = PCA(n_components=2)\n",
    "pca_lda = pca.fit_transform(arr)\n",
    "pca_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "tsne_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)   \n",
    "    y = range(X.shape[0])\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], 'x',\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "plot_embedding(tsne_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_lda[:,0], pca_lda[:,1], color=plt.cm.Set1(range(len(pca_lda))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-61d253c66cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/text/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/text/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m           \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m       \u001b[0mdoc_topic_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/text/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mgammad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mphinorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0mphinorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0;31m# If gamma hasn't changed much, we're done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
