{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19020 entries, 0 to 19019\n",
      "Data columns (total 26 columns):\n",
      "alias               19020 non-null object\n",
      "ratingValue         19020 non-null int64\n",
      "dataPublished       19020 non-null object\n",
      "description         19020 non-null object\n",
      "author              19020 non-null object\n",
      "sentiment           19020 non-null int64\n",
      "word_count          19020 non-null int64\n",
      "sent_count          19020 non-null int64\n",
      "chr_count           19020 non-null int64\n",
      "avg_word_len        19020 non-null float64\n",
      "avg_sent_len        19020 non-null float64\n",
      "num_of_stopwords    19020 non-null int64\n",
      "num_of_modals       19020 non-null int64\n",
      "hashtags            19020 non-null int64\n",
      "mentions            19020 non-null int64\n",
      "numerics            19020 non-null int64\n",
      "uppercase_cnt       19020 non-null int64\n",
      "punctuation_cnt     19020 non-null int64\n",
      "vocab_cnt           19020 non-null int64\n",
      "ratio_lexical       19020 non-null float64\n",
      "ratio_content       19020 non-null float64\n",
      "sent_tokens         19020 non-null object\n",
      "word_tokens_doc     19020 non-null object\n",
      "norm_tokens_doc     19020 non-null object\n",
      "word_tokens         19020 non-null object\n",
      "norm_tokens         19020 non-null object\n",
      "dtypes: float64(4), int64(13), object(9)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('../../data/processed/yp_competitors_rws_0001_0050_basicfeatures.csv')\n",
    "df2 = pd.read_csv('../../data/processed/yp_competitors_rws_0001_0050_textfeatures.csv')\n",
    "df = df1.merge(df2, on=['alias', 'ratingValue', 'dataPublished', 'description', 'author'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fixes\n",
    "from itertools import chain\n",
    "import ast\n",
    "# csv list fix with : ast\n",
    "df.sent_tokens = df.sent_tokens.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "df.word_tokens_doc = df.word_tokens_doc.apply(lambda x: ast.literal_eval(x))\n",
    "df.norm_tokens_doc = df.norm_tokens_doc.apply(lambda x: ast.literal_eval(x))\n",
    "df.word_tokens = df.word_tokens.apply(lambda x: ast.literal_eval(x))\n",
    "df.norm_tokens = df.norm_tokens.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>word_tokens_doc</th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[I stumbled across this great restaurant overl...</td>\n",
       "      <td>[(I, stumbled, across, this, great, restaurant...</td>\n",
       "      <td>[(stumbl, across, great, restaur, overlook, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Excellent view on the ocean at sunset., Excel...</td>\n",
       "      <td>[(Excellent, view, on, the, ocean, at, sunset,...</td>\n",
       "      <td>[(excel, view, ocean, sunset), (excel, food), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[This place was not what the reviews portrayed...</td>\n",
       "      <td>[(This, place, was, not, what, the, reviews, p...</td>\n",
       "      <td>[(place, review, portray), (starter, walk, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[We were excited to repeat our Keoki's (in Kau...</td>\n",
       "      <td>[(We, were, excited, to, repeat, our, Keoki, '...</td>\n",
       "      <td>[(excit, repeat, keoki, 's, kauai, lovefest, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[If you're looking for a tourist spot, this is...</td>\n",
       "      <td>[(If, you, 're, looking, for, a, tourist, spot...</td>\n",
       "      <td>[('re, look, tourist, spot), (unfortun, could,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sent_tokens  \\\n",
       "0  [I stumbled across this great restaurant overl...   \n",
       "1  [Excellent view on the ocean at sunset., Excel...   \n",
       "2  [This place was not what the reviews portrayed...   \n",
       "3  [We were excited to repeat our Keoki's (in Kau...   \n",
       "4  [If you're looking for a tourist spot, this is...   \n",
       "\n",
       "                                     word_tokens_doc  \\\n",
       "0  [(I, stumbled, across, this, great, restaurant...   \n",
       "1  [(Excellent, view, on, the, ocean, at, sunset,...   \n",
       "2  [(This, place, was, not, what, the, reviews, p...   \n",
       "3  [(We, were, excited, to, repeat, our, Keoki, '...   \n",
       "4  [(If, you, 're, looking, for, a, tourist, spot...   \n",
       "\n",
       "                                     norm_tokens_doc  \n",
       "0  [(stumbl, across, great, restaur, overlook, oc...  \n",
       "1  [(excel, view, ocean, sunset), (excel, food), ...  \n",
       "2  [(place, review, portray), (starter, walk, sta...  \n",
       "3  [(excit, repeat, keoki, 's, kauai, lovefest, s...  \n",
       "4  [('re, look, tourist, spot), (unfortun, could,...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['sent_tokens', 'word_tokens_doc', 'norm_tokens_doc']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>dataPublished</th>\n",
       "      <th>description</th>\n",
       "      <th>author</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>chr_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>...</th>\n",
       "      <th>uppercase_cnt</th>\n",
       "      <th>punctuation_cnt</th>\n",
       "      <th>vocab_cnt</th>\n",
       "      <th>ratio_lexical</th>\n",
       "      <th>ratio_content</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>word_tokens_doc</th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>norm_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-01-06</td>\n",
       "      <td>I stumbled across this great restaurant overlo...</td>\n",
       "      <td>Bella L.</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>11</td>\n",
       "      <td>664</td>\n",
       "      <td>4.022222</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>76</td>\n",
       "      <td>0.637037</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>[I stumbled across this great restaurant overl...</td>\n",
       "      <td>[(I, stumbled, across, this, great, restaurant...</td>\n",
       "      <td>[(stumbl, across, great, restaur, overlook, oc...</td>\n",
       "      <td>[I, stumbled, across, this, great, restaurant,...</td>\n",
       "      <td>[stumbl, across, great, restaur, overlook, oce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>Excellent view on the ocean at sunset.\\nExcell...</td>\n",
       "      <td>Rachou A.</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>160</td>\n",
       "      <td>3.611111</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>[Excellent view on the ocean at sunset., Excel...</td>\n",
       "      <td>[(Excellent, view, on, the, ocean, at, sunset,...</td>\n",
       "      <td>[(excel, view, ocean, sunset), (excel, food), ...</td>\n",
       "      <td>[Excellent, view, on, the, ocean, at, sunset, ...</td>\n",
       "      <td>[excel, view, ocean, sunset, excel, food, fres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>This place was not what the reviews portrayed ...</td>\n",
       "      <td>Ozzetta B.</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>14</td>\n",
       "      <td>1229</td>\n",
       "      <td>3.596364</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>130</td>\n",
       "      <td>0.534545</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>[This place was not what the reviews portrayed...</td>\n",
       "      <td>[(This, place, was, not, what, the, reviews, p...</td>\n",
       "      <td>[(place, review, portray), (starter, walk, sta...</td>\n",
       "      <td>[This, place, was, not, what, the, reviews, po...</td>\n",
       "      <td>[place, review, portray, starter, walk, stair,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-08</td>\n",
       "      <td>We were excited to repeat our Keoki's (in Kaua...</td>\n",
       "      <td>Arleen C.</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "      <td>34</td>\n",
       "      <td>2226</td>\n",
       "      <td>3.783158</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>206</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.604211</td>\n",
       "      <td>[We were excited to repeat our Keoki's (in Kau...</td>\n",
       "      <td>[(We, were, excited, to, repeat, our, Keoki, '...</td>\n",
       "      <td>[(excit, repeat, keoki, 's, kauai, lovefest, s...</td>\n",
       "      <td>[We, were, excited, to, repeat, our, Keoki, 's...</td>\n",
       "      <td>[excit, repeat, keoki, 's, kauai, lovefest, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>If you're looking for a tourist spot, this is ...</td>\n",
       "      <td>Carol B.</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>10</td>\n",
       "      <td>776</td>\n",
       "      <td>3.732143</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>97</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.636905</td>\n",
       "      <td>[If you're looking for a tourist spot, this is...</td>\n",
       "      <td>[(If, you, 're, looking, for, a, tourist, spot...</td>\n",
       "      <td>[('re, look, tourist, spot), (unfortun, could,...</td>\n",
       "      <td>[If, you, 're, looking, for, a, tourist, spot,...</td>\n",
       "      <td>['re, look, tourist, spot, unfortun, could, n'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                alias  ratingValue dataPublished  \\\n",
       "0  kimos-maui-lahaina            5    2019-01-06   \n",
       "1  kimos-maui-lahaina            5    2019-01-04   \n",
       "2  kimos-maui-lahaina            3    2018-12-25   \n",
       "3  kimos-maui-lahaina            2    2018-12-08   \n",
       "4  kimos-maui-lahaina            3    2018-11-29   \n",
       "\n",
       "                                         description      author  sentiment  \\\n",
       "0  I stumbled across this great restaurant overlo...    Bella L.          1   \n",
       "1  Excellent view on the ocean at sunset.\\nExcell...   Rachou A.          1   \n",
       "2  This place was not what the reviews portrayed ...  Ozzetta B.          0   \n",
       "3  We were excited to repeat our Keoki's (in Kaua...   Arleen C.          0   \n",
       "4  If you're looking for a tourist spot, this is ...    Carol B.          0   \n",
       "\n",
       "   word_count  sent_count  chr_count  avg_word_len  \\\n",
       "0         135          11        664      4.022222   \n",
       "1          36           5        160      3.611111   \n",
       "2         275          14       1229      3.596364   \n",
       "3         475          34       2226      3.783158   \n",
       "4         168          10        776      3.732143   \n",
       "\n",
       "                         ...                          uppercase_cnt  \\\n",
       "0                        ...                                     10   \n",
       "1                        ...                                      0   \n",
       "2                        ...                                      5   \n",
       "3                        ...                                      7   \n",
       "4                        ...                                      1   \n",
       "\n",
       "   punctuation_cnt  vocab_cnt  ratio_lexical  ratio_content  \\\n",
       "0               17         76       0.637037       0.674074   \n",
       "1                6         25       0.777778       0.638889   \n",
       "2               31        130       0.534545       0.567273   \n",
       "3               43        206       0.484211       0.604211   \n",
       "4               21         97       0.648810       0.636905   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [I stumbled across this great restaurant overl...   \n",
       "1  [Excellent view on the ocean at sunset., Excel...   \n",
       "2  [This place was not what the reviews portrayed...   \n",
       "3  [We were excited to repeat our Keoki's (in Kau...   \n",
       "4  [If you're looking for a tourist spot, this is...   \n",
       "\n",
       "                                     word_tokens_doc  \\\n",
       "0  [(I, stumbled, across, this, great, restaurant...   \n",
       "1  [(Excellent, view, on, the, ocean, at, sunset,...   \n",
       "2  [(This, place, was, not, what, the, reviews, p...   \n",
       "3  [(We, were, excited, to, repeat, our, Keoki, '...   \n",
       "4  [(If, you, 're, looking, for, a, tourist, spot...   \n",
       "\n",
       "                                     norm_tokens_doc  \\\n",
       "0  [(stumbl, across, great, restaur, overlook, oc...   \n",
       "1  [(excel, view, ocean, sunset), (excel, food), ...   \n",
       "2  [(place, review, portray), (starter, walk, sta...   \n",
       "3  [(excit, repeat, keoki, 's, kauai, lovefest, s...   \n",
       "4  [('re, look, tourist, spot), (unfortun, could,...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [I, stumbled, across, this, great, restaurant,...   \n",
       "1  [Excellent, view, on, the, ocean, at, sunset, ...   \n",
       "2  [This, place, was, not, what, the, reviews, po...   \n",
       "3  [We, were, excited, to, repeat, our, Keoki, 's...   \n",
       "4  [If, you, 're, looking, for, a, tourist, spot,...   \n",
       "\n",
       "                                         norm_tokens  \n",
       "0  [stumbl, across, great, restaur, overlook, oce...  \n",
       "1  [excel, view, ocean, sunset, excel, food, fres...  \n",
       "2  [place, review, portray, starter, walk, stair,...  \n",
       "3  [excit, repeat, keoki, 's, kauai, lovefest, si...  \n",
       "4  ['re, look, tourist, spot, unfortun, could, n'...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document is Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [stumbl, across, great, restaur, overlook, oce...\n",
       "1    [excel, view, ocean, sunset, excel, food, fres...\n",
       "2    [place, review, portray, starter, walk, stair,...\n",
       "3    [excit, repeat, keoki, 's, kauai, lovefest, si...\n",
       "4    ['re, look, tourist, spot, unfortun, could, n'...\n",
       "Name: norm_tokens, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documents are reviews\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "data = df.norm_tokens[:100]\n",
    "\n",
    "id2word = Dictionary(documents=data)\n",
    "doc_term_matrix = [id2word.doc2bow(doc) for doc in data]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "lda_model = Lda(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=id2word,\n",
    "    num_topics=10, \n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.101426671191072 , 0.27825371332849375\n"
     ]
    }
   ],
   "source": [
    "# lda test\n",
    "perplexity = lda_model.log_perplexity(doc_term_matrix)\n",
    "coherence = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v').get_coherence()\n",
    "print(perplexity, ',', coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32474995085991565"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimal topic numbers\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "for num_topics in range(2, 40, 4):\n",
    "    model = Lda(\n",
    "        corpus=doc_term_matrix,\n",
    "        id2word=id2word,\n",
    "        num_topics=num_topics, \n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True\n",
    "    )\n",
    "    model_list.append(model)\n",
    "    coherencemodel = CoherenceModel(model=model, texts=data, dictionary=id2word, coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "optimal_model, optimal_coherence = max(((i,y) for i,y in zip(model_list, coherence_values)), key=lambda x: x[1])\n",
    "optimal_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x1a36a0ecf0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((i,y) for i,y in zip(model_list, coherence_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to select number of topics\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(2, 40, 4)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '../../model/mallet-2.0.8/bin/mallet.bat'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []; b = []\n",
    "for doc_topic_list in lda_model.get_document_topics(doc_term_matrix):\n",
    "    dtp = max(doc_topic_list, key=lambda x: x[1])\n",
    "    a.append(dtp)\n",
    "    doc_topic_list.remove(dtp)\n",
    "    b.append(doc_topic_list)\n",
    "pd.concat([df.iloc[:100,:], pd.DataFrame(a)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document is Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "data = pd.Series(list(chain(*df.norm_tokens_doc)), name='norm_tokens_doc')\n",
    "\n",
    "id2word = Dictionary(documents=data)\n",
    "doc_term_matrix = [id2word.doc2bow(doc) for doc in data]\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "lda_model = Lda(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=id2word,\n",
    "    num_topics=10, \n",
    "    random_state=100,\n",
    "    update_every=1, # online iterative learning\n",
    "    chunksize=100,\n",
    "    passes=5,\n",
    "    distributed=False,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.038*\"beach\" + 0.032*\"also\" + 0.030*\"lahaina\" + 0.028*\"made\"'),\n",
       " (1, '0.066*\"like\" + 0.053*\"best\" + 0.045*\"tri\" + 0.035*\"littl\"'),\n",
       " (2, '0.069*\"back\" + 0.069*\"u\" + 0.057*\"delici\" + 0.028*\"sauc\"'),\n",
       " (3, '0.122*\"food\" + 0.108*\"good\" + 0.046*\"would\" + 0.028*\"price\"'),\n",
       " (4, '0.064*\"maui\" + 0.037*\"come\" + 0.030*\"meal\" + 0.030*\"tabl\"'),\n",
       " (5, '0.064*\"...\" + 0.043*\"even\" + 0.040*\"wait\" + 0.037*\"amaz\"'),\n",
       " (6, '0.088*\"great\" + 0.066*\"place\" + 0.062*\"servic\" + 0.046*\"one\"'),\n",
       " (7, '0.145*\"\\'s\" + 0.070*\"time\" + 0.031*\"ruth\" + 0.027*\"went\"'),\n",
       " (8, '0.095*\"pizza\" + 0.063*\"order\" + 0.042*\"taco\" + 0.033*\"steak\"'),\n",
       " (9, '0.110*\"n\\'t\" + 0.059*\"get\" + 0.047*\"realli\" + 0.043*\"nice\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(num_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.038*\"beach\" + 0.032*\"also\" + 0.030*\"lahaina\" + 0.028*\"made\"'),\n",
       " (1, '0.066*\"like\" + 0.053*\"best\" + 0.045*\"tri\" + 0.035*\"littl\"'),\n",
       " (2, '0.069*\"back\" + 0.069*\"u\" + 0.057*\"delici\" + 0.028*\"sauc\"'),\n",
       " (3, '0.122*\"food\" + 0.108*\"good\" + 0.046*\"would\" + 0.028*\"price\"'),\n",
       " (4, '0.064*\"maui\" + 0.037*\"come\" + 0.030*\"meal\" + 0.030*\"tabl\"'),\n",
       " (5, '0.064*\"...\" + 0.043*\"even\" + 0.040*\"wait\" + 0.037*\"amaz\"'),\n",
       " (6, '0.088*\"great\" + 0.066*\"place\" + 0.062*\"servic\" + 0.046*\"one\"'),\n",
       " (7, '0.145*\"\\'s\" + 0.070*\"time\" + 0.031*\"ruth\" + 0.027*\"went\"'),\n",
       " (8, '0.095*\"pizza\" + 0.063*\"order\" + 0.042*\"taco\" + 0.033*\"steak\"'),\n",
       " (9, '0.110*\"n\\'t\" + 0.059*\"get\" + 0.047*\"realli\" + 0.043*\"nice\"')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = '../../model/lda_model_0001_0050_simple_doc2bow'\n",
    "# save\n",
    "lda_model.save(fname)\n",
    "\n",
    "# load\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "saved_model = Lda.load(fname)\n",
    "saved_model.show_topics(num_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19020"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.norm_tokens_doc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136033"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda_model.get_document_topics(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(stumbl, across, great, restaur, overlook, oce...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.181670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(high, expect, place, boy, blow, water)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.186504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(fish, chip, best, 've, ever, 've, lot, includ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(highli, recommend)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.174916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(also, turkey, bacon, sandwich, good)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.142321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     norm_tokens_doc  topic      prob\n",
       "0  (stumbl, across, great, restaur, overlook, oce...      5  0.181670\n",
       "1            (high, expect, place, boy, blow, water)      2  0.186504\n",
       "2  (fish, chip, best, 've, ever, 've, lot, includ...      1  0.237983\n",
       "3                                (highli, recommend)      3  0.174916\n",
       "4              (also, turkey, bacon, sandwich, good)      6  0.142321"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score topic and probabilities to each document (sentence in this case)\n",
    "a = []; b = []\n",
    "for doc_topic_list in lda_model.get_document_topics(doc_term_matrix):\n",
    "    dtp = max(doc_topic_list, key=lambda x: x[1])\n",
    "    a.append(dtp)\n",
    "    doc_topic_list.remove(dtp)\n",
    "    b.append(doc_topic_list)\n",
    "\n",
    "pd_sent_topic = pd.concat([data, pd.DataFrame(a, columns=['topic', 'prob'])], axis=1)\n",
    "pd_sent_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "norm_tokens_doc    136033\n",
       "topic              136033\n",
       "prob               136033\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sent_topic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.07618688),\n",
       " (1, 0.29323208),\n",
       " (2, 0.058359765),\n",
       " (3, 0.11239939),\n",
       " (4, 0.0752151),\n",
       " (5, 0.069250196),\n",
       " (6, 0.09821621),\n",
       " (7, 0.062355176),\n",
       " (8, 0.08586889),\n",
       " (9, 0.06891633)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document/review topic prob distribution\n",
    "lda_model[doc_term_matrix][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.07618688),\n",
       " (1, 0.29323208),\n",
       " (2, 0.058359765),\n",
       " (3, 0.11239939),\n",
       " (4, 0.0752151),\n",
       " (5, 0.069250196),\n",
       " (6, 0.09821621),\n",
       " (7, 0.062355176),\n",
       " (8, 0.08586889),\n",
       " (9, 0.06891633)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document/review topic prob distribution\n",
    "lda_model.get_document_topics(doc_term_matrix[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000223517418"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(w for i, w in lda_model[doc_term_matrix][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stumbl',\n",
       " 'across',\n",
       " 'great',\n",
       " 'restaur',\n",
       " 'overlook',\n",
       " 'ocean',\n",
       " 'lunch',\n",
       " 'vacat',\n",
       " 'maui')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]\n",
    "# df.norm_tokens_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stumbl',\n",
       "  'across',\n",
       "  'great',\n",
       "  'restaur',\n",
       "  'overlook',\n",
       "  'ocean',\n",
       "  'lunch',\n",
       "  'vacat',\n",
       "  'maui'),\n",
       " ('high', 'expect', 'place', 'boy', 'blow', 'water'),\n",
       " ('fish', 'chip', 'best', \"'ve\", 'ever', \"'ve\", 'lot', 'includ', 'london'),\n",
       " ('highli', 'recommend'),\n",
       " ('also', 'turkey', 'bacon', 'sandwich', 'good'),\n",
       " ('term', 'drink', 'highli', 'recommend', 'pacif', 'paradis', 'drink'),\n",
       " ('delici', 'tropic'),\n",
       " ('also', 'realli', 'enjoy', 'lahaina', 'lemonad'),\n",
       " ('servic', 'realli', 'great'),\n",
       " ('wish',\n",
       "  'rememb',\n",
       "  'waitress',\n",
       "  'name',\n",
       "  'truli',\n",
       "  'awesom',\n",
       "  'recommend',\n",
       "  'best',\n",
       "  'stuff'),\n",
       " ('blond', 'cute', 'sunglass')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in df.norm_tokens_doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136033"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd_sent_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136033"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(i) for i in df.norm_tokens_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 14, 34, 10, 6, 6, 14, 8, 10]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in df.norm_tokens_doc[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(stumbl, across, great, restaur, overlook, oce...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.181670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(high, expect, place, boy, blow, water)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.186504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(fish, chip, best, 've, ever, 've, lot, includ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(highli, recommend)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.174916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(also, turkey, bacon, sandwich, good)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.142321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(term, drink, highli, recommend, pacif, paradi...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.297764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(delici, tropic)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.142629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(also, realli, enjoy, lahaina, lemonad)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.149166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(servic, realli, great)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.185749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(wish, rememb, waitress, name, truli, awesom, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.293232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(blond, cute, sunglass)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.155992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      norm_tokens_doc  topic      prob\n",
       "0   (stumbl, across, great, restaur, overlook, oce...      5  0.181670\n",
       "1             (high, expect, place, boy, blow, water)      2  0.186504\n",
       "2   (fish, chip, best, 've, ever, 've, lot, includ...      1  0.237983\n",
       "3                                 (highli, recommend)      3  0.174916\n",
       "4               (also, turkey, bacon, sandwich, good)      6  0.142321\n",
       "5   (term, drink, highli, recommend, pacif, paradi...      3  0.297764\n",
       "6                                    (delici, tropic)      2  0.142629\n",
       "7             (also, realli, enjoy, lahaina, lemonad)      0  0.149166\n",
       "8                             (servic, realli, great)      6  0.185749\n",
       "9   (wish, rememb, waitress, name, truli, awesom, ...      1  0.293232\n",
       "10                            (blond, cute, sunglass)      5  0.155992"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sent_topic.iloc[0:11,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(excel, view, ocean, sunset)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.146975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(excel, food)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.139928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(fresh, fish, coconut, yuzu, husband)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.229818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(love,)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.126704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(waitress, super, nice)</td>\n",
       "      <td>9</td>\n",
       "      <td>0.150506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          norm_tokens_doc  topic      prob\n",
       "11           (excel, view, ocean, sunset)      6  0.146975\n",
       "12                          (excel, food)      3  0.139928\n",
       "13  (fresh, fish, coconut, yuzu, husband)      8  0.229818\n",
       "14                                (love,)      6  0.126704\n",
       "15                (waitress, super, nice)      9  0.150506"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sent_topic.iloc[11:16,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(place, review, portray)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.156842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(starter, walk, stair, want, sit, balconi, cat...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.226674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(told, ask, sit, tabl, clean, coupl, left)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.210729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(wait, anoth, server, walk, right, u, sat, ano...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.257854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(end, wait, anoth, tabl, place, 40, min, tabl,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.237178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(food, okay, ..., veget, option, ravioli, dish...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.196095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(tofu, although, look, like, nice, grill, piec...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.219524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>('ve, tofu, plenti, time, actual, get, pick, f...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.204731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(true, dish)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.176876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(husband, fish, dish, rice, veggi, side, sweet...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.356887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(unfortun, hash, thing, like)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.132608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>('ve, come, maui, almost, 8, year, like, tri, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.257103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(first, time, tri, kimo, 's, unfortun, last)</td>\n",
       "      <td>7</td>\n",
       "      <td>0.244741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(sorry..l, ob)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.130495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      norm_tokens_doc  topic      prob\n",
       "16                           (place, review, portray)      6  0.156842\n",
       "17  (starter, walk, stair, want, sit, balconi, cat...      8  0.226674\n",
       "18         (told, ask, sit, tabl, clean, coupl, left)      2  0.210729\n",
       "19  (wait, anoth, server, walk, right, u, sat, ano...      5  0.257854\n",
       "20  (end, wait, anoth, tabl, place, 40, min, tabl,...      4  0.237178\n",
       "21  (food, okay, ..., veget, option, ravioli, dish...      8  0.196095\n",
       "22  (tofu, although, look, like, nice, grill, piec...      3  0.219524\n",
       "23  ('ve, tofu, plenti, time, actual, get, pick, f...      9  0.204731\n",
       "24                                       (true, dish)      8  0.176876\n",
       "25  (husband, fish, dish, rice, veggi, side, sweet...      8  0.356887\n",
       "26                      (unfortun, hash, thing, like)      8  0.132608\n",
       "27  ('ve, come, maui, almost, 8, year, like, tri, ...      1  0.257103\n",
       "28       (first, time, tri, kimo, 's, unfortun, last)      7  0.244741\n",
       "29                                     (sorry..l, ob)      6  0.130495"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sent_topic.iloc[16:30,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sentences and their topic/probs for the **first review**\n",
    "pd_sent_topic[pd_sent_topic.norm_tokens_doc.apply(lambda x: tuple(x)).isin([i for i in df.norm_tokens_doc[0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [norm_tokens_doc, topic, prob]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_doc_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6,4,6,5,6,6,3,6,3,1,8'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(map(str, pd_doc_temp.topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# This is buggy, because the .isin(inner_list) select multiple occurances of inner_list and list populates with\n",
    "# out of document sentences..\n",
    "# 03/20/2019\n",
    "#\n",
    "topic_mode = []\n",
    "topic_list = []\n",
    "sent_topics = []\n",
    "# score topic and probabilities to each document (**review** in this case)\n",
    "for doc_sents in df.norm_tokens_doc[:100]:    \n",
    "    # pd_doc_temp: the topic and the prob for the current document/ review: all sentences and their topic and probs\n",
    "    pd_doc_temp = pd_sent_topic[pd_sent_topic.norm_tokens_doc.apply(lambda x: tuple(x)).isin([tuple(i) for i in doc_sents])]\n",
    "    #Â find the most frequent topic in the review\n",
    "    sent_topics.append(pd_doc_temp.apply(lambda x: (x.topic, x.prob), axis=1).tolist())\n",
    "    topic_mode.append(round(pd_doc_temp.topic.mode()[0]))    \n",
    "    topic_list.append(','.join(map(str, pd_doc_temp.topic)))\n",
    "    \n",
    "df['sent_topics'] = pd.Series(sent_topics, name='sent_topics')\n",
    "df['topic_mode'] = pd.Series(topic_mode, name='topic_mode')\n",
    "df['topic_list'] = pd.Series(topic_list, name='topic_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# This works since we are using indices of current document on the lda's documents\n",
    "# 03/21/2019\n",
    "#\n",
    "topic_mode = []\n",
    "topic_list = []\n",
    "sent_topics = []\n",
    "i, j = 0, 0\n",
    "# score topic and probabilities to each document (**review** in this case)\n",
    "for doc_sents in df.norm_tokens_doc:\n",
    "    j = j + len(doc_sents)\n",
    "    pd_doc_temp = pd_sent_topic.iloc[i:j,:]\n",
    "    i = j\n",
    "    \n",
    "    sent_topics.append(pd_doc_temp.apply(lambda x: (x.topic, x.prob), axis=1).tolist())\n",
    "    topic_mode.append(round(pd_doc_temp.topic.mode()[0]))    \n",
    "    topic_list.append(','.join(map(str, pd_doc_temp.topic)))\n",
    "    \n",
    "df['sent_topics'] = pd.Series(sent_topics, name='sent_topics')\n",
    "df['topic_mode'] = pd.Series(topic_mode, name='topic_mode')\n",
    "df['topic_list'] = pd.Series(topic_list, name='topic_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136022    (6, 0.14720681309700012)\n",
       "136023    (4, 0.14807088673114777)\n",
       "136024     (6, 0.1423206925392151)\n",
       "136025    (5, 0.15356479585170746)\n",
       "136026    (6, 0.18574859201908112)\n",
       "136027    (6, 0.17367175221443176)\n",
       "136028    (3, 0.12663573026657104)\n",
       "136029    (6, 0.13902904093265533)\n",
       "136030    (3, 0.13209319114685059)\n",
       "136031    (1, 0.17586977779865265)\n",
       "136032    (8, 0.17008419334888458)\n",
       "dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_doc_temp.apply(lambda x: (x.topic, x.prob), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "      <th>ratingValue</th>\n",
       "      <th>word_tokens_doc</th>\n",
       "      <th>topic_mode</th>\n",
       "      <th>topic_list</th>\n",
       "      <th>sent_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>5</td>\n",
       "      <td>[(I, stumbled, across, this, great, restaurant...</td>\n",
       "      <td>1</td>\n",
       "      <td>5,2,1,3,6,3,2,0,6,1,5</td>\n",
       "      <td>[(5, 0.181669682264328), (2, 0.186504259705543...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>5</td>\n",
       "      <td>[(Excellent, view, on, the, ocean, at, sunset,...</td>\n",
       "      <td>6</td>\n",
       "      <td>6,3,8,6,9</td>\n",
       "      <td>[(6, 0.146974578499794), (3, 0.139927998185157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>3</td>\n",
       "      <td>[(This, place, was, not, what, the, reviews, p...</td>\n",
       "      <td>8</td>\n",
       "      <td>6,8,2,5,4,8,3,9,8,8,8,1,7,6</td>\n",
       "      <td>[(6, 0.15684203803539276), (8, 0.2266744673252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>2</td>\n",
       "      <td>[(We, were, excited, to, repeat, our, Keoki, '...</td>\n",
       "      <td>2</td>\n",
       "      <td>2,4,9,0,5,3,7,1,0,4,0,5,5,8,6,9,2,8,6,2,2,1,6,...</td>\n",
       "      <td>[(2, 0.1735130101442337), (4, 0.13328838348388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kimos-maui-lahaina</td>\n",
       "      <td>3</td>\n",
       "      <td>[(If, you, 're, looking, for, a, tourist, spot...</td>\n",
       "      <td>6</td>\n",
       "      <td>6,9,8,6,4,0,5,3,1,9</td>\n",
       "      <td>[(6, 0.1469745934009552), (9, 0.20430350303649...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                alias  ratingValue  \\\n",
       "0  kimos-maui-lahaina            5   \n",
       "1  kimos-maui-lahaina            5   \n",
       "2  kimos-maui-lahaina            3   \n",
       "3  kimos-maui-lahaina            2   \n",
       "4  kimos-maui-lahaina            3   \n",
       "\n",
       "                                     word_tokens_doc  topic_mode  \\\n",
       "0  [(I, stumbled, across, this, great, restaurant...           1   \n",
       "1  [(Excellent, view, on, the, ocean, at, sunset,...           6   \n",
       "2  [(This, place, was, not, what, the, reviews, p...           8   \n",
       "3  [(We, were, excited, to, repeat, our, Keoki, '...           2   \n",
       "4  [(If, you, 're, looking, for, a, tourist, spot...           6   \n",
       "\n",
       "                                          topic_list  \\\n",
       "0                              5,2,1,3,6,3,2,0,6,1,5   \n",
       "1                                          6,3,8,6,9   \n",
       "2                        6,8,2,5,4,8,3,9,8,8,8,1,7,6   \n",
       "3  2,4,9,0,5,3,7,1,0,4,0,5,5,8,6,9,2,8,6,2,2,1,6,...   \n",
       "4                                6,9,8,6,4,0,5,3,1,9   \n",
       "\n",
       "                                         sent_topics  \n",
       "0  [(5, 0.181669682264328), (2, 0.186504259705543...  \n",
       "1  [(6, 0.146974578499794), (3, 0.139927998185157...  \n",
       "2  [(6, 0.15684203803539276), (8, 0.2266744673252...  \n",
       "3  [(2, 0.1735130101442337), (4, 0.13328838348388...  \n",
       "4  [(6, 0.1469745934009552), (9, 0.20430350303649...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['alias', 'ratingValue', 'word_tokens_doc', 'topic_mode', 'topic_list', 'sent_topics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/processed/yp_competitors_rws_0001_0050_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_tokens_doc</th>\n",
       "      <th>topic</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136022</th>\n",
       "      <td>(husband, surpris, birthday, cake, kristi)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.147207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136023</th>\n",
       "      <td>(last, minut, abl, make, happen)</td>\n",
       "      <td>4</td>\n",
       "      <td>0.148071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136024</th>\n",
       "      <td>(unfortun, n't, like, pineappl, cake)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.142321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136025</th>\n",
       "      <td>(thought, kind, dri, side, also, may, fridg, d...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.153565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136026</th>\n",
       "      <td>(cake, gluten, free)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.185749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136027</th>\n",
       "      <td>('m, sure, realiz, specialti, bakeri)</td>\n",
       "      <td>6</td>\n",
       "      <td>0.173672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136028</th>\n",
       "      <td>(like, carrot, cake, 's, fave)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.126636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136029</th>\n",
       "      <td>(anyway, sweet, gestur, husband, 's, part, eve...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.139029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136030</th>\n",
       "      <td>(thank, kristi, short, notic, cake)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.132093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136031</th>\n",
       "      <td>('m, sure, could, 've, gotten, someth, like, w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136032</th>\n",
       "      <td>(cream, chees, frost)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.170084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          norm_tokens_doc  topic      prob\n",
       "136022         (husband, surpris, birthday, cake, kristi)      6  0.147207\n",
       "136023                   (last, minut, abl, make, happen)      4  0.148071\n",
       "136024              (unfortun, n't, like, pineappl, cake)      6  0.142321\n",
       "136025  (thought, kind, dri, side, also, may, fridg, d...      5  0.153565\n",
       "136026                               (cake, gluten, free)      6  0.185749\n",
       "136027              ('m, sure, realiz, specialti, bakeri)      6  0.173672\n",
       "136028                     (like, carrot, cake, 's, fave)      3  0.126636\n",
       "136029  (anyway, sweet, gestur, husband, 's, part, eve...      6  0.139029\n",
       "136030                (thank, kristi, short, notic, cake)      3  0.132093\n",
       "136031  ('m, sure, could, 've, gotten, someth, like, w...      1  0.175870\n",
       "136032                              (cream, chees, frost)      8  0.170084"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_doc_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[doc_term_matrix]):\n",
    "    topic_weights.append([w for i, w in row_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "arr.shape\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "# topic_num = np.argmax(arr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a 2d PCA model to the vectors\n",
    "pca = PCA(n_components=2)\n",
    "pca_lda = pca.fit_transform(arr)\n",
    "pca_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "tsne_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)   \n",
    "    y = range(X.shape[0])\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], 'x',\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "plot_embedding(tsne_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_lda[:,0], pca_lda[:,1], color=plt.cm.Set1(range(len(pca_lda))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
